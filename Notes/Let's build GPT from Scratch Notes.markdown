# ğŸ“š Let's build GPT: from scratch ç¬”è®°

## ğŸ¯ ç›®æ ‡

ä»é›¶å®ç°GPTï¼ˆç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼‰ï¼Œç†è§£Transformerè§£ç å™¨æ¶æ„ï¼Œæ„å»ºä¸€ä¸ªå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ã€‚

- **èƒŒæ™¯** ğŸ”: ä»makemoreç³»åˆ—å‡çº§ï¼ŒWaveNetå¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼Œä½†Transformeræ›´é€‚åˆå¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ã€‚
- **é‡ç‚¹** ğŸŒŸ: å®ç°GPTçš„æ ¸å¿ƒç»„ä»¶ï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ›ã€FFNã€å±‚å½’ä¸€åŒ–ï¼‰ï¼Œè®­ç»ƒä¸€ä¸ªç®€å•å­—ç¬¦çº§æ¨¡å‹ã€‚

---

## ğŸ§  GPT ä»£ç ç»“æ„ä¸æµç¨‹

### 1ï¸âƒ£ æ•°æ®å‡†å¤‡

- **ä»»åŠ¡**: ä½¿ç”¨æ–‡æœ¬æ•°æ®é›†ï¼ˆå¦‚Tiny Shakespeareï¼‰ï¼Œæ„å»ºå­—ç¬¦çº§è¾“å…¥-è¾“å‡ºå¯¹ã€‚

- **ä»£ç **:
  - è¯»å–æ–‡æœ¬ï¼Œåˆ›å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆè¯æ±‡è¡¨ï¼Œå¦‚65ä¸ªå­—ç¬¦ï¼‰ã€‚
  - è¾“å…¥ï¼šä¸Šä¸‹æ–‡åºåˆ—ï¼ˆ`block_size`å­—ç¬¦ï¼‰ï¼Œè¾“å‡ºï¼šä¸‹ä¸€å­—ç¬¦ã€‚

- **å½¢è±¡åŒ–**:

  ```
  æ–‡æœ¬: "to be or not"
  è¾“å…¥: [t,o, ] â†’ è¾“å‡º: b
  ```

- **å›¾æ ‡**: ğŸ“œ æ•°æ®åƒâ€œæ–‡æœ¬çš„å­—ç¬¦æµâ€ã€‚

### 2ï¸âƒ£ Transformeræ¶æ„

- **æ ¸å¿ƒç»„ä»¶**:

  - **åµŒå…¥å±‚**: å­—ç¬¦ç´¢å¼• â†’ åµŒå…¥å‘é‡ï¼ˆ`n_embd`ï¼Œå¦‚384ç»´ï¼‰ã€‚
  - **ä½ç½®ç¼–ç **: ä¸ºæ¯ä¸ªä½ç½®æ·»åŠ å›ºå®šæˆ–å¯å­¦ä¹ çš„å‘é‡ã€‚
  - **å¤šå¤´è‡ªæ³¨æ„åŠ›**: æ•æ‰å­—ç¬¦é—´ä¾èµ–ï¼Œå…è®¸å¹¶è¡Œè®¡ç®—ã€‚
  - **å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰**: æ¯ä½ç½®çš„éçº¿æ€§å˜æ¢ã€‚
  - **å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰**: ç¨³å®šè®­ç»ƒã€‚
  - **æ®‹å·®è¿æ¥**: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€‚

- **ä»£ç **:

  ```python
  class GPT(nn.Module):
      def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):
          self.token_emb = nn.Embedding(vocab_size, n_embd)
          self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))
          self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)])
          self.ln_f = nn.LayerNorm(n_embd)
          self.head = nn.Linear(n_embd, vocab_size)

      def forward(self, idx):
          tok_emb = self.token_emb(idx)  # (B, T, C)
          pos_emb = self.pos_emb[:, :idx.size(1), :]  # (1, T, C)
          x = tok_emb + pos_emb  # (B, T, C)
          for block in self.blocks: x = block(x)
          x = self.ln_f(x)
          logits = self.head(x)  # (B, T, vocab_size)
          return logits
  ```

- **å½¢è±¡åŒ–**:

  ```
  è¾“å…¥ â†’ åµŒå…¥+ä½ç½®ç¼–ç  â†’ Nä¸ªTransformerå— â†’ LayerNorm â†’ çº¿æ€§å±‚ â†’ logits
  ```

- **å›¾æ ‡**: ğŸ—ï¸ Transformeråƒâ€œæ¨¡å—åŒ–çš„è¯­è¨€å¤„ç†å·¥å‚â€ã€‚

### 3ï¸âƒ£ è‡ªæ³¨æ„åŠ›æœºåˆ¶

- **ä½œç”¨**: æ¯ä¸ªå­—ç¬¦æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚

- **ä»£ç **:

  ```python
  class Head(nn.Module):
      def __init__(self, head_size):
          self.key = nn.Linear(n_embd, head_size, bias=False)
          self.query = nn.Linear(n_embd, head_size, bias=False)
          self.value = nn.Linear(n_embd, head_size, bias=False)
          self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

      def forward(self, x):
          k = self.key(x)    # (B, T, head_size)
          q = self.query(x)  # (B, T, head_size)
          wei = q @ k.transpose(-2, -1) * head_size**-0.5  # (B, T, T)
          wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # å› æœæ©ç 
          wei = F.softmax(wei, dim=-1)
          v = self.value(x)  # (B, T, head_size)
          out = wei @ v      # (B, T, head_size)
          return out
  ```

- **å½¢è±¡åŒ–**:

  ```
  æŸ¥è¯¢(Q) Ã— é”®(K) â†’ æ³¨æ„åŠ›æƒé‡ â†’ æ©ç  â†’ softmax â†’ åŠ æƒå€¼(V)
  ```

- **å›¾æ ‡**: ğŸ” è‡ªæ³¨æ„åŠ›åƒâ€œä¸Šä¸‹æ–‡çš„åŠ¨æ€èšç„¦â€ã€‚

### 4ï¸âƒ£ è®­ç»ƒä¸ç”Ÿæˆ

- **è®­ç»ƒ**:

  - æŸå¤±ï¼šäº¤å‰ç†µï¼ˆ`F.cross_entropy`ï¼‰ã€‚
  - ä¼˜åŒ–å™¨ï¼šAdamï¼Œå­¦ä¹ ç‡ï¼ˆå¦‚3e-4ï¼‰ã€‚
  - æ•°æ®ï¼šmini-batchå¤„ç†ã€‚

- **ç”Ÿæˆ**:

  - ä»åˆå§‹ä¸Šä¸‹æ–‡å¼€å§‹ï¼Œé‡‡æ ·logitsï¼Œç”Ÿæˆæ–°å­—ç¬¦ã€‚

- **å›¾æ ‡**: ğŸ¨ ç”Ÿæˆåƒâ€œæ¨¡å‹çš„æ–‡æœ¬åˆ›ä½œâ€ã€‚

---

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ

- **è‡ªæ³¨æ„åŠ›** ğŸ”—: åŠ¨æ€å»ºæ¨¡å­—ç¬¦é—´ä¾èµ–ï¼Œå…è®¸å¹¶è¡Œè®¡ç®—ã€‚
- **å› æœæ©ç ** ğŸ›‘: ç¡®ä¿åªå…³æ³¨å†å²å­—ç¬¦ï¼Œé€‚åˆç”Ÿæˆä»»åŠ¡ã€‚
- **æ®‹å·®è¿æ¥** â•: ç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ã€‚
- **å±‚å½’ä¸€åŒ–** ğŸ›¡ï¸: æ ‡å‡†åŒ–æ¿€æ´»ï¼Œç¨³å®šè®­ç»ƒã€‚
- **ä½ç½®ç¼–ç ** ğŸ“: æä¾›åºåˆ—é¡ºåºä¿¡æ¯ã€‚

---

## ğŸ› ï¸ å®ç”¨æŠ€å·§

- **è¶…å‚æ•°** âš™ï¸: è°ƒæ•´`n_embd`ï¼ˆ384ï¼‰ã€`n_head`ï¼ˆ6ï¼‰ã€`n_layer`ï¼ˆ6ï¼‰ã€`block_size`ï¼ˆ128ï¼‰ã€‚
- **åˆå§‹åŒ–** ğŸ› ï¸: æƒé‡ç”¨å°æ ‡å‡†å·®ï¼Œé˜²æ­¢åˆå§‹æŸå¤±è¿‡é«˜ã€‚
- **ç”Ÿæˆå¤šæ ·æ€§** ğŸŒˆ: ç”¨`temperature`æ§åˆ¶é‡‡æ ·éšæœºæ€§ã€‚
- **è°ƒè¯•** ğŸ”: æ£€æŸ¥æ³¨æ„åŠ›æƒé‡ï¼Œç¡®ä¿å› æœæ©ç æ­£ç¡®ã€‚

---

## ğŸ“š èµ„æº

- **è§†é¢‘**: Let's build GPT: from scratch, in code, spelled out
- **ä»£ç **: nn-zero-to-hero GitHub
- **è®ºæ–‡**: Vaswani et al. (2017)ã€ŠAttention is All You Needã€‹
- **Colab**: è§†é¢‘æè¿°ä¸­çš„Jupyterç¬”è®°æœ¬

---

## ğŸŒŸ æ€»ç»“

ä»é›¶å®ç°GPTï¼Œæ ¸å¿ƒæ˜¯Transformerè§£ç å™¨ï¼ˆè‡ªæ³¨æ„åŠ›+FFN+æ®‹å·®+LayerNormï¼‰ã€‚ä»£ç æ¨¡å—åŒ–ï¼Œæ¸…æ™°å±•ç¤ºä»åµŒå…¥åˆ°ç”Ÿæˆçš„å…¨æµç¨‹ï¼Œå­—ç¬¦çº§å»ºæ¨¡ä¸ºç†è§£GPTå¥ å®šåŸºç¡€ã€‚