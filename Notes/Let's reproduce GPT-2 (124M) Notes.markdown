# ğŸ“š Let's reproduce GPT-2 (124M) ç¬”è®°

## ğŸ¯ ç›®æ ‡

å¤ç°OpenAIçš„GPT-2ï¼ˆ124Må‚æ•°ï¼‰ï¼Œä»å¤´è®­ç»ƒä¸€ä¸ªå­è¯çº§è¯­è¨€æ¨¡å‹ï¼Œç†è§£å¤§è§„æ¨¡Transformerçš„å®ç°ä¸è®­ç»ƒã€‚

- **èƒŒæ™¯** ğŸ”: GPTä»å­—ç¬¦çº§å‡çº§åˆ°å­è¯çº§ï¼ŒGPT-2æ˜¯å¼ºå¤§ä¸”å¼€æºçš„è¯­è¨€æ¨¡å‹ã€‚
- **é‡ç‚¹** ğŸŒŸ: å®ç°GPT-2æ¶æ„ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå¾®è°ƒå¹¶ç”Ÿæˆæ–‡æœ¬ã€‚

---

## ğŸ§  GPT-2 (124M) ä»£ç ç»“æ„ä¸æµç¨‹

### 1ï¸âƒ£ æ¨¡å‹æ¶æ„

- **å‚æ•°**:

  - è¯æ±‡è¡¨ï¼š50,257ï¼ˆBPEï¼‰ã€‚
  - åµŒå…¥ç»´åº¦ï¼š768ã€‚
  - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š1,024ã€‚
  - å±‚æ•°ï¼š12ã€‚
  - æ³¨æ„åŠ›å¤´æ•°ï¼š12ã€‚
  - å‚æ•°é‡ï¼šçº¦1.24äº¿ã€‚

- **ä»£ç **:

  ```python
  class GPT2(nn.Module):
      def __init__(self):
          super().__init__()
          self.transformer = nn.ModuleDict(dict(
              wte = nn.Embedding(vocab_size, n_embd),
              wpe = nn.Embedding(block_size, n_embd),
              h = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)]),
              ln_f = nn.LayerNorm(n_embd),
          ))
          self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
  
      def forward(self, idx):
          tok_emb = self.transformer.wte(idx)  # (B, T, n_embd)
          pos = torch.arange(0, idx.size(1), device=idx.device)
          pos_emb = self.transformer.wpe(pos)  # (T, n_embd)
          x = tok_emb + pos_emb
          for block in self.transformer.h: x = block(x)
          x = self.transformer.ln_f(x)
          logits = self.lm_head(x)  # (B, T, vocab_size)
          return logits
  ```

- **å½¢è±¡åŒ–**:

  ```
  è¾“å…¥ â†’ è¯åµŒå…¥+ä½ç½®åµŒå…¥ â†’ 12ä¸ªTransformerå— â†’ LayerNorm â†’ çº¿æ€§å¤´ â†’ logits
  ```

- **å›¾æ ‡**: ğŸ° GPT-2åƒâ€œå¤§è§„æ¨¡è¯­è¨€å¤„ç†åŸå ¡â€ã€‚

### 2ï¸âƒ£ Tokenizer

- **å®ç°**: ä½¿ç”¨é¢„è®­ç»ƒBPE Tokenizerï¼ˆ50,257è¯æ±‡ï¼‰ã€‚

- **ä»£ç **:

  - åŠ è½½Hugging Faceçš„`gpt2` Tokenizeræˆ–è‡ªå®šä¹‰BPEã€‚
  - ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—ï¼Œè§£ç å›æ–‡æœ¬ã€‚

- **å›¾æ ‡**: ğŸ”„ Tokenizeråƒâ€œå­è¯çš„ç¼–ç å™¨â€ã€‚

### 3ï¸âƒ£ è®­ç»ƒæ•°æ®

- **æ•°æ®é›†**: WebTextæˆ–å…¬å¼€æ–‡æœ¬ï¼ˆå¦‚Wikipediaï¼‰ã€‚

- **å¤„ç†**:

  - ç¼–ç ä¸ºtokenåºåˆ—ï¼Œåˆ‡åˆ†ä¸º`block_size`ï¼ˆ1,024ï¼‰ã€‚
  - æ„å»ºmini-batchï¼ˆå¦‚batch_size=8ï¼‰ã€‚

- **å›¾æ ‡**: ğŸ“š æ•°æ®åƒâ€œäº’è”ç½‘çš„æ–‡æœ¬å®åº“â€ã€‚

### 4ï¸âƒ£ è®­ç»ƒæµç¨‹

- **æŸå¤±**: äº¤å‰ç†µï¼Œé¢„æµ‹ä¸‹ä¸€tokenã€‚

- **ä¼˜åŒ–å™¨**: AdamWï¼Œå­¦ä¹ ç‡ï¼ˆå¦‚6e-4ï¼‰ï¼Œå¸¦æƒé‡è¡°å‡ã€‚

- **åˆ†å¸ƒå¼è®­ç»ƒ**:

  - æ•°æ®å¹¶è¡Œï¼ˆå¤šGPUåˆ†æ‹…batchï¼‰ã€‚
  - æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batchã€‚

- **ä»£ç **:

  ```python
  model = GPT2().to(device)
  optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)
  for step in range(max_steps):
      xb, yb = get_batch('train')
      logits = model(xb)
      loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  ```

- **å›¾æ ‡**: ğŸš€ è®­ç»ƒåƒâ€œæ¨¡å‹çš„æ·±åº¦å­¦ä¹ é©¬æ‹‰æ¾â€ã€‚

### 5ï¸âƒ£ åŠ è½½é¢„è®­ç»ƒæƒé‡

- **æ–¹æ³•**: ä»Hugging FaceåŠ è½½GPT-2 (124M)æƒé‡ã€‚

- **ä»£ç **:

  - æ˜ å°„æƒé‡åˆ°è‡ªå®šä¹‰æ¨¡å‹ï¼ˆ`wte` â†’ è¯åµŒå…¥ï¼Œ`wpe` â†’ ä½ç½®åµŒå…¥ï¼‰ã€‚
  - éªŒè¯å‰å‘ä¼ æ’­ä¸€è‡´æ€§ã€‚

- **å›¾æ ‡**: ğŸ“¦ é¢„è®­ç»ƒæƒé‡åƒâ€œæ¨¡å‹çš„çŸ¥è¯†åº“â€ã€‚

### 6ï¸âƒ£ ç”Ÿæˆæ–‡æœ¬

- **æ–¹æ³•**: è‡ªå›å½’é‡‡æ ·ï¼Œtop-kæˆ–top-pï¼ˆnucleusï¼‰é‡‡æ ·ã€‚

- **ä»£ç **:

  ```python
  def generate(idx, max_new_tokens, temperature=1.0):
      for _ in range(max_new_tokens):
          logits = model(idx[:, -block_size:])
          logits = logits[:, -1, :] / temperature
          probs = F.softmax(logits, dim=-1)
          idx_next = torch.multinomial(probs, num_samples=1)
          idx = torch.cat((idx, idx_next), dim=1)
      return idx
  ```

- **å›¾æ ‡**: ğŸ¨ ç”Ÿæˆåƒâ€œæ¨¡å‹çš„è¯­è¨€åˆ›ä½œâ€ã€‚

---

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ

- **å­è¯å»ºæ¨¡** ğŸ§¬: BPE Tokenizerå¹³è¡¡åºåˆ—é•¿åº¦ä¸è¯­ä¹‰è¡¨è¾¾ã€‚
- **å¤§è§„æ¨¡Transformer** ğŸ—ï¸: æ·±å±‚ç½‘ç»œï¼ˆ12å±‚ï¼‰æ•æ‰å¤æ‚è¯­è¨€æ¨¡å¼ã€‚
- **é¢„è®­ç»ƒæƒé‡** ğŸ“š: å¤ç”¨OpenAIçš„æƒé‡ï¼Œå¿«é€Ÿå¯åŠ¨ã€‚
- **é‡‡æ ·ç­–ç•¥** ğŸŒˆ: top-k/top-pæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ä¸è¿è´¯æ€§ã€‚

---

## ğŸ› ï¸ å®ç”¨æŠ€å·§

- **è¶…å‚æ•°** âš™ï¸: å­¦ä¹ ç‡6e-4ï¼Œbatch_size 8ï¼Œä¸Šä¸‹æ–‡1,024ã€‚
- **å†…å­˜ä¼˜åŒ–** ğŸ› ï¸: æ¢¯åº¦ç´¯ç§¯æ”¯æŒå°GPUè®­ç»ƒã€‚
- **ç”Ÿæˆæ§åˆ¶** ğŸ›ï¸: è°ƒæ•´`temperature`ï¼ˆ0.8-1.2ï¼‰æˆ–top-pï¼ˆ0.9ï¼‰ã€‚
- **éªŒè¯** ğŸ”: æ£€æŸ¥é¢„è®­ç»ƒæƒé‡åŠ è½½åæŸå¤±æ˜¯å¦åˆç†ã€‚

---

## ğŸ“š èµ„æº

- **è§†é¢‘**: Let's reproduce GPT-2 (124M)
- **ä»£ç **: nn-zero-to-hero GitHub
- **è®ºæ–‡**: Radford et al. (2019)ã€ŠLanguage Models are Unsupervised Multitask Learnersã€‹
- **Hugging Face**: GPT-2æ¨¡å‹ä¸Tokenizer
- **Colab**: è§†é¢‘æè¿°ä¸­çš„Jupyterç¬”è®°æœ¬

---

## ğŸŒŸ æ€»ç»“

å¤ç°GPT-2 (124M)é€šè¿‡å­è¯Tokenizerå’Œ12å±‚Transformerå®ç°å¼ºå¤§è¯­è¨€å»ºæ¨¡ã€‚ä»£ç ä»æ•°æ®åˆ°è®­ç»ƒå†åˆ°ç”Ÿæˆï¼Œæ¸…æ™°æ¨¡å—åŒ–ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡åŠ é€Ÿå¼€å‘ï¼Œå±•ç°ç°ä»£NLPçš„å¨åŠ›ã€‚