# 📚 makemore Part 2: MLP 笔记

## 🎯 目标
使用多层感知器（MLP）构建字符级语言模型，预测名字中的下一个字符，改进Part 1的Bigram模型。

- **Bigram局限** 🚫: 仅用1个字符作为上下文，预测能力弱，扩展上下文导致参数爆炸。
- **MLP优势** 🌟: 用神经网络处理多字符上下文，学习字符关系，生成更合理名字。

---

## 🧠 MLP 代码结构与流程

### 1️⃣ 数据准备
- **任务**: 从名字数据集中构造输入-输出对（上下文 → 下一字符）。
- **示例**: 上下文窗口为3字符，如“isa”预测“b”（来自“isabella”）。
- **代码逻辑**:
  - 读取名字数据集，构建字符到索引的映射（词汇表，27个字符：a-z + `.`）。
  - 创建训练/验证/测试集（train/dev/test split）📊，避免过拟合。
- **形象化**:
  ```
  名字: isabella
  上下文: [i,s,a] → 预测: b
  ```

### 2️⃣ 嵌入层 (Embedding Layer)
- **作用**: 将字符映射为低维向量（嵌入），捕获字符间的语义关系。
- **代码**:
  - 初始化嵌入表 `C`（形状: [27, n_embd]，如27字符 × 30维）。
  - 3个上下文字符的嵌入拼接为输入向量（3 × 30 = 90维）。
- **形象化**:
  ```
  字符 i → [0.2, -0.1, ...] (30维向量)
  上下文 [i,s,a] → 拼接成 90维输入
  ```
- **图标**: 📍 嵌入就像给每个字符分配一个“坐标”。

### 3️⃣ 隐藏层 (Hidden Layer)
- **作用**: 将嵌入向量变换为高层次特征，增加模型表达力。
- **代码**:
  - 线性层（权重矩阵 + 偏置）+ `tanh` 非线性激活。
  - 超参数：隐藏层神经元数（如200）。
- **形象化**:
  ```
  输入 (90维) → 线性变换 → tanh → 输出 (200维)
  ```
- **图标**: 🛠️ 隐藏层是“特征加工厂”。

### 4️⃣ 输出层 (Output Layer)
- **作用**: 将隐藏层输出映射到词汇表大小（27），预测下一字符概率。
- **代码**:
  - 线性层生成logits（27维）。
  - 用 `F.cross_entropy` 计算损失（结合softmax和负对数似然）。
- **形象化**:
  ```
  隐藏层 (200维) → 线性变换 → logits (27维) → softmax → 概率
  ```
- **图标**: 🎰 输出层像“概率分配器”。

### 5️⃣ 训练循环
- **流程**:
  1. 前向传播：输入 → 嵌入 → 隐藏层 → 输出层 → 损失。
  2. 反向传播：计算梯度，更新参数（`C`, 权重，偏置）。
  3. 优化：用Adam优化器，调整学习率（如0.1 → 0.01）。
- **小批量训练** 🚀: 每次处理部分数据（minibatch），提高效率。
- **验证过拟合** ✅: 在小批量上训练，损失接近0，确保模型能学习。

### 6️⃣ 采样生成
- **作用**: 用训练好的模型生成新名字。
- **代码**:
  - 从初始上下文（如“...”）开始，预测下一字符。
  - 用概率分布采样，迭代生成名字。
- **形象化**:
  ```
  上下文: ... → 预测 p → 采样 r → 新上下文 ..p → 预测 e ...
  输出: preston
  ```
- **图标**: 🎨 采样像“随机创作”。

---

## 🔑 核心概念
- **嵌入** 📍: 字符的向量表示，模型学习字符间的相似性（如元音靠近）。
- **损失函数** 📉: 负对数似然（NLL），衡量预测概率与真实字符的差距。
- **数据集划分** 📊:
  - 训练集：优化参数。
  - 验证集：调超参数。
  - 测试集：评估性能。
- **超参数调优** ⚙️:
  - 嵌入维度（30 → 50）。
  - 隐藏层大小（200 → 300）。
  - 学习率（0.1 → 0.01）。
- **可视化** 📈: 绘制嵌入向量，观察字符关系（如元音、辅音聚类）。

---

## 🛠️ 实用技巧
- **学习率** ⚡: 初始高（0.1），后期降低（0.01），加速收敛。
- **过拟合检查** 🔍: 在小批量上训练，确认损失接近0。
- **批量大小** 📦: 平衡速度与稳定性（如32或64）。
- **可视化嵌入** 🌐: 理解模型学到的字符关系。

---

## 📚 资源
- **视频**: [Building makemore Part 2: MLP](https://www.youtube.com/watch?v=TCH_1BHY58I)
- **代码**: [nn-zero-to-hero GitHub](https://github.com/karpathy/nn-zero-to-hero)
- **论文**: Bengio et al. (2003)《A Neural Probabilistic Language Model》
- **Colab**: 视频描述中的Jupyter笔记本

---

## 🌟 总结
MLP通过嵌入层、隐藏层和输出层，利用多字符上下文预测下一字符，显著优于Bigram模型。代码结构清晰，前向传播、反向传播和采样流程直观。通过实验超参数和可视化嵌入，可深入理解模型行为。