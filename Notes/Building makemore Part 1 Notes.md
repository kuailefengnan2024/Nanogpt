# 📚 Make More：字符级语言模型的构建与理解

> Make More 是一个能够“生成更多类似输入内容”的模型，类似于前面的 Micrograd 项目。本节课重点讲解如何一步步构建字符级语言模型。

## 🎯 Make More 简介

*   **Make More** 的名字含义是“生成更多的东西”。
*   训练数据示例是 **names.txt**，包含大约 32,000 个名字，来源于政府网站的随机收集。
*   训练后的模型能生成类似人名的独特名称，比如：**Dontel, Irot, Zhendi** 等。
*   生成的名字听起来像真实名字，但实际上是新的、独特的字符序列。

## 🧠 字符级语言模型（Character-level Language Model）

> 字符级语言模型是以字符为单位进行建模的语言模型，目标是预测给定字符序列的下一个字符。

*   训练数据的每一行是一个名字，视为一个字符序列。
*   例如，“reese”被拆分成字符序列：r, e, e, s, e。
*   重点在于学习字符之间的顺序关系及概率分布。
*   训练模型的任务是根据已有字符预测下一个最可能出现的字符。

## 🤖 计划实现的模型架构

|                                   |                     |                     |
| --------------------------------- | ------------------- | ------------------- |
| 模型类型                              | 简介                  | 备注                  |
| 二元语法模型（Bigram Model）              | 仅考虑前一个字符预测下一个字符     | 最简单的语言模型，局部结构预测     |
| 多层感知机（Multilayer Perceptrons）     | 使用神经网络实现，学习更复杂的字符关系 | 能捕捉非线性关系            |
| 循环神经网络（Recurrent Neural Networks） | 处理序列数据，考虑长距离依赖      | 适合语言序列建模            |
| 变换器模型（Transformer）                | 基于注意力机制的现代网络        | 目标实现类似 GPT-2 的字符级模型 |

## 📊 数据加载与预处理

1.  **加载数据：** 读取 `names.txt` 文件，将所有名字读取为一个大字符串。
2.  **分割为单词列表：** 使用字符串的 `splitlines()` 方法，将大字符串拆分成名字列表。
3.  **查看数据：** 查看前 10 个名字，例如：Emma, Olivia, Eva 等。
4.  **数据统计：**

|        |          |
| ------ | -------- |
| 属性     | 值        |
| 总名字数   | 约 32,000 |
| 最短名字长度 | 2 个字符    |
| 最长名字长度 | 15 个字符   |

## 🔍 理解字符序列中的统计信息

*   一个单词（如 "isabella"）不仅仅是一个整体，它隐含了多个统计信息：

    *   字符 **i** 可能是名字的开头字符。
    *   字符 **s** 很可能跟在 **i** 后面。
    *   字符 **a** 很可能跟在 **is** 后面。
    *   字符 **b** 很可能跟在 **isa** 后面。
    *   直到单词结尾，模型还需判断何时停止。

*   因此，单个单词中包含了字符序列的概率信息以及序列终止信息。

*   数据集中有 32,000 个这样的单词，蕴含大量概率结构。

## 🧩 二元语法模型 (Bigram Model)

> 二元语法模型只考虑相邻的两个字符，预测下一个字符只依赖于当前字符。

*   输入：当前字符
*   输出：下一个字符的概率分布
*   优点：简单，易于实现
*   缺点：只能捕捉局部的、相邻字符的关系，忽略更长距离依赖

## 💻 二元语法模型的实现细节

### 遍历单词生成二元组

    for c1, c2 in zip(w, w[1:]):
        print(c1, c2)

|       |       |
| ----- | ----- |
| 第一个字符 | 第二个字符 |
| e     | m     |
| m     | m     |
| m     | a     |

*   `zip(w, w[1:])` 的作用是将字符串 `w` 与其从第二个字符开始的切片配对，形成连续字符对。

### 添加特殊起始和结束符号

*   为了捕捉单词的开始和结束信息，加入：

|                  |           |
| ---------------- | --------- |
| 符号类型             | 作用        |
| 开始符（start token） | 表示单词的起始位置 |
| 结束符（end token）   | 表示单词的结束位置 |

*   例如，单词 "emma" 变成：`[start, e, m, m, a, end]`。
*   这样，bigram 包含了 `(start, e)` 和 `(a, end)`，帮助模型了解序列边界。

## 📈 统计二元组出现频率

*   创建一个字典 `b`，用来存储每个二元组出现的次数。

|                                    |                |
| ---------------------------------- | -------------- |
| 代码示例                               | 作用             |
| `b = {}`                           | 初始化空字典         |
| `b[bigram] = b.get(bigram, 0) + 1` | 统计二元组出现次数，默认 0 |

*   统计完成后，字典 `b` 包含所有二元组及其对应的出现次数，例如：

|            |      |
| ---------- | ---- |
| 二元组        | 出现次数 |
| (start, e) | 5000 |
| (e, m)     | 3000 |
| (a, end)   | 4500 |
| ...        | ...  |

*   通过统计整个数据集，得到大规模的二元组出现频率分布。

## 🔄 处理完整数据集

*   统计过程对所有 32,000 个单词执行，累积所有二元组的出现次数。
*   统计结果为后续模型训练提供概率基础。

## 📝 重要代码片段回顾

    # 假设 words 是名字列表
    b = {}

    for w in words:
        chars = ['<start>'] + list(w) + ['<end>']
        for c1, c2 in zip(chars, chars[1:]):
            bigram = (c1, c2)
            b[bigram] = b.get(bigram, 0) + 1

*   该代码完成了对所有单词的二元组频率统计。

## ✨ 小结

*   **Make More** 是一个基于字符级语言模型的名字生成器。
*   从加载数据、理解字符序列统计信息、实现二元语法模型、统计二元组频率，逐步建立语言模型基础。
*   后续将实现更复杂的模型（多层感知机、循环神经网络、变换器），最终构建类似 GPT-2 的字符级生成模型。

# 🧮 大小写字母双字母组合（bigram）计数与二维数组表示

> 本讲主要讨论如何对文本中的双字母组合进行统计，并将统计结果由字典转换为二维数组（tensor 张量）表示，利用 PyTorch 库高效操作和可视化。

## 双字母组合计数与排序

    sorted(b.items(), key=lambda kv: kv[1], reverse=True)

## 从字典到二维数组的转换

### 为什么要转换？

*   字典结构不便于高效批量运算和直观可视化。
*   二维数组（矩阵）更适合表示双字母组合统计，行对应第一个字符，列对应第二个字符。
*   使用深度学习框架 PyTorch 的 `torch.tensor` 来创建和操作二维数组。

### PyTorch 张量基础

|        |                              |                                            |
| ------ | ---------------------------- | ------------------------------------------ |
| 操作     | 说明                           | 示例代码                                       |
| 创建零矩阵  | 创建指定大小的全零张量                  | `a = torch.zeros(3, 5)`                    |
| 指定数据类型 | 默认是 `float32`，可以指定为整数类型      | `a = torch.zeros(3, 5, dtype=torch.int32)` |
| 索引赋值   | 使用行列索引修改元素值，索引从0开始           | `a[1, 3] = 1`                              |
| 读取元素   | 读取元素值后用 `.item()` 转为Python整数 | `val = a[1, 3].item()`                     |

### 构建大数组

*   字母表包括26个小写字母和两个特殊字符 `s` 和 `e`，共28个字符。
*   创建大小为 $28 \times 28$ 的二维张量 `N` 来统计双字母组合出现次数。

## 字符到索引的映射（字符编码）

*   字符串不能直接作为数组索引，需用整数索引。
*   通过构造 **查找表** 实现字符到整数的映射（`s2i`）和反向映射（`i2s`）：

|       |                   |
| ----- | ----------------- |
| 映射类型  | 作用                |
| `s2i` | 字符 → 索引（整数）       |
| `i2s` | 索引（整数） → 字符（反向映射） |

*   构造步骤：

    1.  将所有单词拼接成一个大字符串。
    2.  使用 `set()` 取唯一字符集合。
    3.  将集合转换为排序列表（按字母顺序）。
    4.  利用 `enumerate()` 给字符赋序号。
    5.  特殊字符 `s` 编号为 26，`e` 编号为 27。

## 更新二维数组计数代码示例

    x1 = s2i[char1]
    x2 = s2i[char2]
    N[x1, x2] += 1

*   其中 `char1` 和 `char2` 是双字母组合的两个字符。
*   `N` 是二维张量，`N[i, j]` 表示字符索引为 `i` 后跟字符索引为 `j` 的双字母组合出现次数。

## 可视化数组矩阵

*   使用 `matplotlib` 库中的 `plt.imshow()` 可视化二维数组。

*   可视化效果包含：

    *   矩阵中每个元素对应的颜色深浅表示出现次数多少。
    *   在每个格子中添加双字母组合字符串及出现次数数字。

*   显示时需要用到 `i2s` 的反向映射将索引转为字符。

## 观察与改进

|                         |                             |
| ----------------------- | --------------------------- |
| 现象                      | 说明                          |
| 某些行全为零                  | 结尾字符 `e` 不可能作为双字母组合第一个字符出现。 |
| 某些列全为零                  | 起始字符 `s` 不可能作为双字母组合第二个字符出现。 |
| 特殊字符 `s` 和 `e` 会使矩阵显得拥挤 | 采用两个特殊字符导致矩阵大小和表示效率不理想。     |

### 改进方案

*   只使用一个特殊字符 `.`（点）替代之前的 `s` 和 `e`，矩阵大小改为 $27 \times 27$。
*   将这个特殊字符索引设为0，其他字母从1开始编号。
*   这样矩阵更紧凑，结构更清晰。

## 特殊字符映射调整示例

|     |     |     |
| --- | --- | --- |
| 字符  | 旧编号 | 新编号 |
| `.` | -   | 0   |
| `a` | 0   | 1   |
| `b` | 1   | 2   |
| ... | ... | ... |
| `z` | 25  | 26  |

*   调整后的映射保证了矩阵的第一行和第一列对应特殊字符 `.`。
*   这样可以更直观地表示单词的起始和结束位置。

## 重要代码片段总结

|          |                                                             |              |
| -------- | ----------------------------------------------------------- | ------------ |
| 功能       | 代码示例                                                        | 说明           |
| 创建零矩阵    | `N = torch.zeros(28, 28, dtype=torch.int32)`                | 28×28 全零整数矩阵 |
| 字符转索引映射  | `s2i = {s:i for i, s in enumerate(sorted_chars)}`           | 字符到索引的字典     |
| 特殊字符手动赋值 | `s2i['s'] = 26; s2i['e'] = 27`                              | 额外添加特殊字符索引   |
| 双字母计数更新  | `N[s2i[c1], s2i[c2]] += 1`                                  | 更新双字母组合计数    |
| 反向索引映射   | `i2s = {i:s for s, i in s2i.items()}`                       | 索引转字符        |
| 可视化矩阵    | `plt.imshow(N, cmap='Blues')`                               | 使用蓝色调显示矩阵    |
| 在格子内显示文本 | `plt.text(j, i, f"{i2s[i]}{i2s[j]}\n{N[i,j].item()}", ...)` | 显示双字母组合和对应计数 |

## 关键概念

> 双字母组合（bigram）：文本中相邻两个字符组成的组合，用于统计语言模型中的字符共现概率。

> 张量（tensor）：多维数组，是PyTorch中数据的基本结构，支持高效的数值计算。

> 字符编码映射：将字符转换为整数索引，使其可用于数组索引操作。

## 代码索引与矩阵索引说明

|              |                                        |
| ------------ | -------------------------------------- |
| 概念           | 说明                                     |
| 字符编码映射 `s2i` | 字符转整数索引，用于数组行列索引                       |
| 反向映射 `i2s`   | 整数索引转字符，用于可视化显示                        |
| 二维数组索引       | `N[i, j]` 表示第 `i` 个字符后跟第 `j` 个字符的双字母计数 |

## 注意事项

*   PyTorch 张量索引返回仍是张量对象，需用 `.item()` 提取标量值。
*   特殊字符的设计关系到矩阵的结构和空间利用率。
*   可视化中，行表示第一个字符，列表示第二个字符，左上角为特殊字符 `.`。

以上内容涵盖了本次讲座讨论的所有技术细节和代码实现，助于深入理解字符双字母组合的统计及其高效表示方式。

# 🧩 大字符二元模型（Bigram Character-Level Language Model）采样详解

> 本节内容讲述了如何基于大字符二元模型进行字符采样的整个流程，包括从计数矩阵到概率分布的转换，以及如何用PyTorch中的torch.multinomial函数进行随机采样。

## 1. 大字符二元模型的计数矩阵结构

|                        |                                      |
| ---------------------- | ------------------------------------ |
| 内容                     | 说明                                   |
| **起始符号（**`.`**）**      | 用作单词起始标志，采样时从该符号对应的行开始。              |
| **计数矩阵（counts array）** | 一个二维数组，行表示当前字符，列表示下一个字符，元素表示字符对出现次数。 |
| **首字母计数**              | 计数矩阵的第一行统计所有单词开头字符的出现次数。             |
| **字符转移结构**             | 中间行统计各字符后面紧跟的字符的出现次数。                |

## 2. 从计数矩阵到概率分布的转换

*   **提取行向量**\
    例如，取第0行即起始符号行，得到一个长为27（字符集大小）的向量，表示每个字符作为首字母的计数。
*   **类型转换与归一化**\
    将计数由整数转换为浮点数，再除以该行计数总和，得到概率分布向量 $p$，满足： $$\sum\_i p\_i = 1$$
*   **意义**\
    该概率分布表示在当前字符状态下，每个后续字符被选择的概率。

## 3. 使用 `torch.multinomial` 进行采样

### 3.1 函数作用

*   `torch.multinomial`：从给定的概率分布中随机采样索引，采样结果符合概率分布。

### 3.2 重要参数说明

|               |                                           |
| ------------- | ----------------------------------------- |
| 参数            | 说明                                        |
| `input`       | 概率分布张量（浮点型），例如上文中的概率向量 $p$。               |
| `num_samples` | 采样数量，例如采样1个字符或20个字符。                      |
| `replacement` | 是否有放回采样，默认是`False`，需要设为`True`以允许多次采样同一元素。 |
| `generator`   | 伪随机数生成器，保证采样的确定性（同一种子下结果一致）。              |

### 3.3 采样流程示例

    g = torch.Generator().manual_seed(1234)

## 4. 生成名字的采样循环逻辑

|                |                                    |
| -------------- | ---------------------------------- |
| 过程步骤           | 说明                                 |
| 初始化索引 `ix = 0` | 从起始符号（`.`）对应的行开始采样。                |
| 采样概率向量 `p`     | 取当前索引 `ix` 对应的行，归一化后得到概率分布。        |
| 采样下一个字符索引 `ix` | 使用 `torch.multinomial` 从概率分布中采样得到。 |
| 判断是否结束         | 若采样到的索引为0（结束符），则终止循环。              |
| 输出字符           | 根据索引映射得到对应字符，追加到结果列表。              |
| 循环继续           | 用新采样的索引作为当前索引，继续采样下一个字符。           |

## 5. 采样结果分析与模型局限

*   **结果示例**\
    生成的名字类似于“mory.”，“yanu o'reilly”等，但整体质量较差，名字不自然。

*   **原因分析**\
    大字符二元模型只考虑了当前字符和下一个字符的关系，没有上下文信息，导致生成的名字没有语义连贯性。

*   **对比实验**

    *   **均匀分布采样**：将概率分布设置为均匀分布，所有字符均等可能，生成结果更加随机且无意义。
    *   **训练模型采样**：基于真实计数的概率分布采样，生成结果虽不完美，但比均匀分布更符合单词构成规律。

## 6. 关键代码片段示例

    ix = 0  # 起始符号索引
    out = []
    g = torch.Generator().manual_seed(1234)  # 确定性随机生成器

    while True:
        p = counts_array[ix].float()
        p /= p.sum()  # 归一化为概率
        sample = torch.multinomial(p, 1, replacement=True, generator=g)
        ix = sample.item()
        if ix == 0:  # 遇到结束符
            break
        out.append(i2s[ix])  # i2s为索引到字符的映射

    name = ''.join(out)
    print(name)

## 7. 表格总结：采样流程与重要函数对比

|          |                   |                                                          |                   |
| -------- | ----------------- | -------------------------------------------------------- | ----------------- |
| 阶段       | 操作内容              | 关键函数/变量                                                  | 说明                |
| 初始化      | 设定起始索引0，创建随机数生成器  | `ix = 0`, `torch.Generator()`                            | 保证采样从起始符号开始且结果可复现 |
| 获取计数行    | 取出当前索引对应的计数行      | `counts_array[ix]`                                       | 包含当前字符后续可能字符的计数   |
| 归一化为概率   | 将计数转为浮点数并除以总和     | `.float()`, `p /= p.sum()`                               | 得到合法的概率分布，和为1     |
| 采样下一字符索引 | 使用多项式采样函数采样一个字符索引 | `torch.multinomial(p, 1, replacement=True, generator=g)` | 根据概率分布采样，支持有放回采样  |
| 判断是否结束   | 判断采样结果是否为结束符索引0   | `if ix == 0: break`                                      | 遇到结束符停止采样循环       |
| 输出字符     | 将采样字符索引映射为字符并保存   | `i2s[ix]`                                                | 生成单词的字符序列         |

## 8. 术语解释

> 大字符二元模型（Bigram Character-Level Language Model）： 一种基于字符对（即连续两个字符）统计的概率语言模型，用于预测一个字符后面可能出现的下一个字符。

> 多项式采样（Multinomial Sampling）： 根据概率分布从多个可能结果中随机选取一个结果的过程，概率越大被选中的可能性越高。

# 📝 本节核心知识点

*   将字符对计数转换为概率分布是采样的基础。
*   `torch.multinomial` 是实现概率采样的关键函数，需注意参数`replacement`和随机数生成器的使用。
*   通过循环采样字符索引，结合起始符号和结束符，实现名字的生成。
*   大字符二元模型生成的名字质量有限，原因在于模型只考虑了相邻字符间的概率关系，缺乏上下文。
*   均匀概率分布采样作为对比，表现更差，说明模型学习到了部分字符组合规律。

# 🌟 例子回顾

*   **采样第一个字符**\
    从起始符号行（索引0）采样，抽到字符“m”，表示该字符是新名字的首字母。
*   **采样后续字符**\
    使用“m”对应的行采样下一个字符，依此类推，直到采样到结束符（索引0）停止。
*   **采样多次生成多个名字**\
    通过循环多次执行采样过程，得到多个随机名字示例，结果显示大字符二元模型比均匀分布生成的名字更合理一些。

# 🚩 注意事项

*   采样前必须将计数转换为概率分布，且概率和应为1。
*   多项式采样时设置`replacement=True`，避免采样错误。
*   使用随机数生成器保证结果可复现，方便调试和实验对比。
*   模型局限性导致生成结果质量有限，需理解模型统计性质。

# 🧮 矩阵归一化与广播机制详解

> 本讲重点介绍了如何对计数矩阵（counts matrix）进行概率归一化处理，避免重复计算，并深入探讨了张量（tensor）操作中的广播（broadcasting）规则。

## 1. 矩阵归一化的效率问题及改进方法

*   **当前问题**：

    *   每次循环时都从计数矩阵中取出一行，转换为浮点数，进行归一化（除以行和），导致重复计算，非常低效且浪费资源。

*   **改进方案**：

    *   预先计算出一个概率矩阵 $P$，其形状同计数矩阵 $N$ 一致。
    *   $P$ 中每一行代表给定前一个字符的条件概率分布，即每行的和为1。
    *   这样后续循环可直接使用 $P$ 的对应行，避免重复归一化。

## 2. 归一化的具体实现步骤

|                |                                        |
| -------------- | -------------------------------------- |
| 步骤             | 说明                                     |
| 复制计数矩阵为浮点型 $P$ | 将整数计数矩阵 $N$ 复制为浮点型矩阵 $P$，以进行概率计算。      |
| 计算每行的和         | 利用 `torch.sum` 计算每一行的元素和，作为归一化的分母。     |
| 按行归一化          | 将 $P$ 中每行的元素除以对应行和，使得每行元素和为1，得到条件概率分布。 |

## 3. `torch.sum` 函数及其参数详解

*   使用 `torch.sum(input, dim, keepdim)`：

    *   **input**：输入张量（矩阵）。
    *   **dim**：指定在哪个维度上求和。
    *   **keepdim**：是否保留求和后被压缩的维度（若为`True`，该维度变为1；若为`False`，该维度被移除）。

*   **举例说明**：

|            |            |         |           |                     |
| ---------- | ---------- | ------- | --------- | ------------------- |
| 输入矩阵形状 $P$ | 求和维度 $dim$ | keepdim | 结果形状      | 说明                  |
| $(27, 27)$ | 0          | True    | $(1, 27)$ | 对列求和，结果为1行27列的行向量。  |
| $(27, 27)$ | 0          | False   | $(27,)$   | 对列求和，结果为一维大小为27的向量。 |
| $(27, 27)$ | 1          | True    | $(27, 1)$ | 对行求和，结果为27行1列的列向量。  |
| $(27, 27)$ | 1          | False   | $(27,)$   | 对行求和，结果为一维大小为27的向量。 |

*   本讲重点用的是 **按行求和**，即 `dim=1`，`keepdim=True`，得到形状 $(27,1)$ 的列向量，方便进行广播操作。

## 4. 广播（Broadcasting）机制详解

> 广播机制是指在进行张量的二元操作（如加法、除法）时，自动扩展较小张量以匹配较大张量形状的规则。

### 广播规则（针对PyTorch）：

1.  **对齐形状**：从张量形状的尾部（右侧）开始对齐维度。

2.  **每个对齐维度满足条件之一**：

    *   两个维度大小相等，
    *   或其中一个维度大小为1，
    *   或维度不存在（即一个张量维度比另一个少，视为1）。

### 应用示例

|            |           |       |                                      |
| ---------- | --------- | ----- | ------------------------------------ |
| 张量A形状      | 张量B形状     | 是否可广播 | 说明                                   |
| $(27, 27)$ | $(27, 1)$ | 是     | 第二维度 $1$ 会被扩展成 $27$，实现逐行除法。          |
| $(27, 27)$ | $(27,)$   | 是     | $(27,)$ 视为 $(1, 27)$，扩展成 $(27, 27)$。 |

### 具体操作解释

*   将 $(27,1)$ 张量复制成 $(27,27)$，每列相同，方便与 $(27,27)$ 张量按元素相除。
*   逐元素除法实现对每一行进行归一化。

## 5. 注意事项与潜在Bug

*   **使用 **`keepdim=True`** 的必要性**：

    *   如果 `keepdim=False`，求和结果会降维成 $(27,)$，形状不再是二维，导致广播逻辑异常。
    *   虽然广播规则理论上允许 $(27,27)$ 与 $(27,)$ 进行操作，但实际执行中会出现错误（bug）。

*   **建议**：

    *   始终使用 `keepdim=True` 保持维度，避免隐藏的广播错误。
    *   认真学习并尊重广播规则，避免随意操作导致结果错误。

## 6. 小结表格：归一化流程与广播

|           |                                         |            |             |
| --------- | --------------------------------------- | ---------- | ----------- |
| 操作步骤      | 代码示例                                    | 结果形状       | 说明          |
| 复制浮点矩阵    | `p = n.float()`                         | $(27, 27)$ | 将计数矩阵转为浮点数  |
| 按行求和，保留维度 | `row_sums = p.sum(dim=1, keepdim=True)` | $(27, 1)$  | 得到列向量，方便广播  |
| 归一化操作     | `p = p / row_sums`                      | $(27, 27)$ | 逐行归一化，每行和为1 |

## 7. 重要概念总结

|           |                                               |
| --------- | --------------------------------------------- |
| 关键词       | 定义                                            |
| **计数矩阵**  | 记录字符对出现次数的二维矩阵，形状为 $(27,27)$，代表字母表大小。         |
| **概率矩阵**  | 由计数矩阵归一化得到的条件概率分布矩阵，每一行表示给定前一个字符后，下一个字符的概率分布。 |
| **广播**    | 自动扩展较小张量维度以匹配较大张量形状的机制，支持元素级运算。               |
| `keepdim` | `torch.sum`的参数，决定是否保留求和后的维度，避免维度丢失导致的广播错误。    |

## 8. 练习建议

*   阅读并理解 PyTorch 官方文档中关于 `torch.sum` 和广播机制的章节。
*   编写代码实践不同维度的张量相除，观察广播行为。
*   分析若不使用 `keepdim=True`，会导致哪些具体错误和异常。

# ⚠️ 代码示例（归一化计数矩阵）

    import torch

    # 假设 n 是 (27, 27) 的计数矩阵
    n = torch.randint(0, 100, (27, 27))

    # 复制为浮点矩阵
    p = n.float()

    # 按行求和，保留维度
    row_sums = p.sum(dim=1, keepdim=True)  # 形状 (27, 1)

    # 逐行归一化
    p = p / row_sums

    # 验证第1行和为1
    assert torch.isclose(p[0].sum(), torch.tensor(1.0))

以上内容覆盖了本次讲座中关于矩阵归一化流程、`torch.sum`用法、广播机制及其潜在陷阱的全部重点。

# ⚙️ 广播机制（Broadcasting）与归一化问题

> 广播机制是指在张量（tensor）运算中，较小维度的张量会自动扩展以匹配较大维度的张量进行逐元素（element-wise）运算的规则。

*   在本节中，讲解了一个具体例子：

    *   有一个形状为 $27$ 的行向量（row vector）。
    *   通过广播规则，行向量被复制成 $27 \times 27$ 的矩阵，进行元素逐一除法。
    *   实际上，发生了对**列的归一化**（normalizing columns），而非预期的对**行的归一化**。

|                      |                                                   |
| -------------------- | ------------------------------------------------- |
| 现象                   | 解释                                                |
| $p\[0]$（第一行）归一化不为 1  | 第一列归一化为1，说明广播方向错了，归一化发生在列上而非行上。                   |
| 维度自动添加               | 广播规则按从右向左对齐维度，缺少维度时自动创建，导致维度转换错误。                 |
| `keepdims=True` 与否影响 | 当 `keepdims=True` 时，维度不丢失，正确保持列向量形状；否则维度丢失导致广播错误。 |

> 关键点：广播操作时，一定要明确维度对齐和是否保持维度，否则容易引入难以察觉的错误。

# 🧮 统计模型参数训练：大ram（bigram）语言模型

> 通过统计计数训练大ram模型，即统计字符对（bigram）出现频率，然后对频率进行归一化，得到条件概率分布。

*   模型参数是一个二维矩阵 $p$，表示每个字符后面接另一个字符的概率。
*   训练过程就是计算这些频率并归一化，确保每行（前一个字符）概率加和为1。
*   该模型可用于生成文本，通过迭代采样下一个字符。

# 🔍 模型质量评估：似然函数与对数似然函数

> 训练完毕后，需要对模型进行评价，衡量模型在训练集上的表现。

## 1. 似然函数（Likelihood）

*   定义为所有训练数据中每个bigram概率的乘积： $$ \text{Likelihood} = \prod\_{i} p\_i $$
*   反映模型对训练数据的整体概率分配能力。
*   似然越大，模型越好。

## 2. 对数似然函数（Log Likelihood）

*   由于概率乘积会非常小，计算不方便，故采用对数变换： $$ \text{Log Likelihood} = \sum\_{i} \log p\_i $$
*   对数函数是单调递增的，保留优化目标。
*   对数似然值范围：当所有概率为1时，最大值为0；概率越小，对数越负。

## 3. 反向负对数似然（Negative Log Likelihood，NLL）

*   负对数似然定义为： $$ \text{NLL} = - \sum\_{i} \log p\_i $$
*   作为损失函数使用，满足“损失越小，模型越好”的直观要求。
*   最小值为0，表示完美预测。

## 4. 平均负对数似然

*   为避免与样本数量相关，通常取平均： $$ \text{平均NLL} = - \frac{1}{N} \sum\_{i=1}^N \log p\_i $$
*   该值即为训练损失，训练目标是最小化平均NLL。

# 📊 概率示例与意义

|                  |                                                |
| ---------------- | ---------------------------------------------- |
| 特征               | 说明                                             |
| 27种可能字符          | 若均匀分布，则概率约为 $\frac{1}{27} \approx 0.037$（3.7%） |
| 观察到模型概率高达40%、35% | 表明模型捕获了训练数据中的统计规律，非均匀分布。                       |
| 较高概率意味着模型预测准确性高  | 尤其是在训练集上，概率接近1说明模型高度拟合训练数据。                    |

# 📝 代码逻辑与示例说明

|                |                                                                     |
| -------------- | ------------------------------------------------------------------- |
| 步骤             | 说明                                                                  |
| 复制代码计算bigram概率 | 通过统计bigram频率生成概率矩阵 $p$，打印部分示例（如单词 "emma", "olivia", "ava" 的bigram）。 |
| 计算每个bigram的概率  | 利用矩阵 $p$ 根据bigram索引获取对应概率，格式化输出显示概率大小。                              |
| 计算对数概率         | 使用 `torch.log()` 对概率取对数，观察对数概率的分布，较大概率对应接近0的对数，较小概率对应负数。            |
| 计算总对数似然        | 对所有bigram对数概率求和，得到训练集的对数似然值（负数）。                                    |
| 计算负对数似然        | 取相反数，使得损失函数越小越好，便于优化。                                               |
| 计算平均负对数似然      | 对负对数似然取平均，作为最终的训练损失值。                                               |

# ⚠️ 广播机制注意事项总结

|                  |                                                        |
| ---------------- | ------------------------------------------------------ |
| 问题点              | 详细说明                                                   |
| 维度自动扩展带来的错误      | 维度不匹配时自动添加新维度，可能导致广播方向错误，归一化方向反转。                      |
| `keepdims` 参数重要性 | 归一化时若不保持维度（`keepdims=True`），结果形状降维，影响广播行为，导致错误。        |
| 广播方向必须明确         | 广播时从右向左对齐维度，错误理解会导致统计归一化方向完全相反。                        |
| 操作效率             | 尽量避免生成新的张量，使用**原地操作（in-place operation）**，提升效率，减少内存占用。 |

# 公式小结

*   似然函数（Likelihood）： $$ L = \prod\_{i=1}^N p\_i $$
*   对数似然函数（Log Likelihood）： $$ \log L = \sum\_{i=1}^N \log p\_i $$
*   负对数似然（Negative Log Likelihood）： $$ \text{NLL} = - \sum\_{i=1}^N \log p\_i $$
*   平均负对数似然： $$ \text{Loss} = - \frac{1}{N} \sum\_{i=1}^N \log p\_i $$

# 💡 关键建议

*   广播操作时务必**检查维度对齐**，避免无意识维度扩展导致错误的归一化方向。
*   使用`keepdims=True`保持维度，确保广播行为符合预期。
*   训练大ram语言模型时，利用统计频率进行概率估计，理解并计算似然及对数似然是评估模型优劣的基础。
*   负对数似然是最常用的损失函数，符合最小化损失的优化目标。
*   代码实现中尽量使用原地操作提升效率，避免不必要的内存复制。

# 📊 语言模型概率参数及优化

> 概率参数是在语言模型中用来表示某事件发生可能性的数值，传统统计语言模型中以表格形式存储。

*   传统方法中概率以表格形式存储（概率矩阵），但未来神经网络模型中，概率将由神经网络计算得到。
*   优化目标是最大化训练数据的**似然函数（likelihood）**，即使模型参数使训练数据出现的概率最大。
*   最大化似然函数等价于最大化**对数似然（log likelihood）**，因为对数函数是单调递增函数，且对数操作将乘积转换为加和，方便计算。

### 对数似然与负对数似然

|                               |          |
| ----------------------------- | -------- |
| 优化目标                          | 说明       |
| 最大化似然 $P(\text{data}          | \theta)$ |
| 最大化对数似然 $\log P(\text{data}   | \theta)$ |
| 最小化负对数似然 $-\log P(\text{data} | \theta)$ |

*   优化时通常计算**平均负对数似然**，得到的数值（例如2.4）衡量模型质量，数值越低模型越好，最低为0。
*   负对数似然隐含模型对训练数据的预测概率越大，损失越小。

# 🧮 语言模型中的概率计算及平滑技术

*   通过训练集统计得到的概率，有些二元组（bigram）可能出现概率为零，导致对数概率为负无穷（$-\infty$），即无限大的损失。
*   例如字符串“andre”和“jq”，模型对“jq”的概率为零，导致对数概率为负无穷，不利于训练和应用。

### 模型平滑（Model Smoothing）

> 模型平滑是通过人为地给所有可能事件加上一个小的伪计数，避免概率为零的技术。

|                 |                               |
| --------------- | ----------------------------- |
| 主要方法            | 说明                            |
| 加1平滑（Laplace平滑） | 给所有计数加1，保证概率矩阵中无零概率           |
| 加大计数            | 加5等较大计数，使模型概率更均匀，减少尖峰，增加模型平滑度 |

*   平滑后概率为零的情况消失，避免了负无穷损失，模型能对所有可能事件给出非零概率。
*   平滑会改变生成的概率分布，有时会使模型预测更均匀，但这是实际应用中可接受的折中。

# 🧩 二元组字符级语言模型训练总结

|           |                           |
| --------- | ------------------------- |
| 过程步骤      | 说明                        |
| 统计所有二元组计数 | 从训练文本中统计相邻字符对出现次数         |
| 归一化       | 将计数行归一化为概率分布，满足概率和为1      |
| 模型评估      | 计算训练集上的负对数似然，评价模型质量       |
| 生成新词      | 根据概率分布采样生成新词，体现模型学习到的语言特征 |

# 🧠 神经网络框架下的二元组字符级语言模型

*   传统统计方法通过计数和归一化得到概率，神经网络方法则通过参数化函数学习概率分布。
*   神经网络模型输入一个字符，输出该字符后续可能出现字符的概率分布。
*   参数为权重矩阵 $W$，通过输入字符编码计算输出概率。

### 训练目标与损失函数

|      |                                             |
| ---- | ------------------------------------------- |
| 内容   | 说明                                          |
| 输入   | 当前字符的整数编码 $x$                               |
| 目标标签 | 下一个字符的整数编码 $y$                              |
| 损失函数 | 平均负对数似然：$-\log p(y                          |
| 优化方法 | **梯度下降（gradient descent）**，调整参数 $W$ 最小化损失函数 |

*   训练数据由大批二元组 $(x,y)$ 组成，输入当前字符预测下一个字符。
*   通过计算预测概率与实际标签的差异，反馈调整神经网络参数。

# 🛠️ 构建神经网络训练集示例

*   训练集由输入列表（当前字符）和标签列表（下一个字符）组成。
*   例子：单词“emma”的字符序列为 `. e e m m a .`，产生5个二元组训练样本：

|            |            |
| ---------- | ---------- |
| 输入字符（整数编码） | 目标字符（整数编码） |
| 0 (对应'.')  | 5 (对应'e')  |
| 5 (对应'e')  | 13 (对应'm') |
| 13 (对应'm') | 13 (对应'm') |
| 13 (对应'm') | 1 (对应'a')  |
| 1 (对应'a')  | 0 (对应'.')  |

*   输入和标签均转化为整数索引，方便神经网络处理。

# ⚠️ 关于 PyTorch 中张量构造的注意事项

|                     |                                       |
| ------------------- | ------------------------------------- |
| 构造函数                | 说明                                    |
| `torch.tensor` (小写) | 自动根据数据类型推断，通常返回浮点型张量（float32），无自动梯度记录 |
| `torch.Tensor` (大写) | 实际为构造器函数，行为稍有不同，可能导致类型不一致，文档不够清晰      |

*   建议统一使用小写的 `torch.tensor` 来构造张量，避免数据类型混乱。
*   注意张量的数据类型对后续神经网络训练和计算有影响。

# 📋 关键术语表

|                                 |                       |
| ------------------------------- | --------------------- |
| 术语                              | 中文解释                  |
| 似然函数 (Likelihood)               | 模型参数使观测数据出现的概率        |
| 对数似然 (Log Likelihood)           | 似然函数的对数，简化乘积为加和，便于计算  |
| 负对数似然 (Negative Log Likelihood) | 损失函数，优化时最小化该值         |
| 模型平滑 (Model Smoothing)          | 给概率计数加伪计数，防止零概率问题     |
| 梯度下降 (Gradient Descent)         | 优化算法，通过损失函数的梯度调整参数    |
| 张量 (Tensor)                     | 多维数组，是神经网络输入输出数据的基本格式 |

# 🔢 重要数学表达式

*   最大化似然： $$\max\_{\theta} \prod\_{i=1}^N P(x\_i | \theta)$$
*   最大化对数似然： $$\max\_{\theta} \sum\_{i=1}^N \log P(x\_i | \theta)$$
*   最小化负对数似然损失： $$\min\_{\theta} -\frac{1}{N} \sum\_{i=1}^N \log P(x\_i | \theta)$$
*   神经网络预测概率： $$p(y | x; W) = \text{softmax}(W \cdot \text{one-hot}(x))$$
*   损失函数： $$\mathcal{L} = -\log p(y | x; W)$$

# ⚙️ 输入数据的整数编码与神经网络输入

> “我们需要将整数（如字符的索引）转换成神经网络能理解的形式，不能直接将整数输入神经网络。”

## 输入整数与神经网络输入的矛盾

*   当前示例中的输入是整数（如0、5、13），代表字符的索引。
*   神经元的输入是通过权重乘以输入的（$w \cdot x + b$），输入如果是整数，会导致计算不合理。
*   神经网络期望的是连续的浮点数输入，而不是离散的整数。

## One-Hot编码（一热编码）

*   **定义**：将整数转换为一个向量，该向量除了对应整数索引的位为1外，其余位均为0。
*   例如，整数13转换成长度为27的一热向量，只有第13个位置是1，其他位置是0。
*   该向量可以直接作为神经网络的输入。

|      |                                        |
| ---- | -------------------------------------- |
| 输入整数 | 对应一热向量（长度27）示例                         |
| 0    | \[1, 0, 0, ..., 0]                     |
| 5    | \[0, 0, 0, 0, 0, 1, 0, ..., 0]         |
| 13   | \[0, 0, ..., 0, 1, 0, ..., 0] (第13位为1) |

## PyTorch中的一热编码实现

*   使用`torch.nn.functional.one_hot`函数

*   传入参数：

    *   整数tensor（类型为Long，即64位整数）
    *   类别数（num\_classes）明确指定编码长度，避免错误猜测

*   例子：

<!---->

    import torch.nn.functional as F
    x_encoded = F.one_hot(x, num_classes=27)

*   编码结果的形状是 `(样本数, 27)`
*   可以用`matplotlib.pyplot.imshow`可视化，查看每个样本对应的向量中哪一位为1。

## 数据类型注意事项

*   一热编码输出默认是整数类型（64位整数）
*   神经网络需要浮点数输入（一般是`float32`）
*   需将编码结果转换为浮点类型：

<!---->

    x_encoded = x_encoded.float()

# 🧠 神经元的构建与矩阵乘法

> “神经元的计算本质是权重和输入的点积加上偏置，这里先考虑无偏置的线性层。”

## 神经元权重初始化

*   使用正态分布随机初始化权重
*   使用`torch.randn`生成正态分布随机数
*   权重形状：27 × 1（输入维度为27，输出维度为1）

<!---->

    w = torch.randn(27, 1)

*   正态分布概率密度函数特点：

    *   大多数值接近0
    *   极少数值大于3或小于-3

## 神经元的矩阵乘法计算

*   输入编码$x\_{encoded}$形状为 $5 \times 27$（5个样本，27维输入）
*   权重$w$形状为 $27 \times 1$
*   矩阵乘法：

$$ \text{output} = x\_{encoded} \times w $$

*   结果形状为 $5 \times 1$，对应5个样本经过单个神经元的输出
*   这里没有加偏置项，只是$w \cdot x$部分

## 批量并行计算多个神经元

*   目标：用27个神经元同时处理输入
*   权重矩阵$w$形状变为 $27 \times 27$
*   输入仍为 $5 \times 27$
*   矩阵乘法后输出为 $5 \times 27$，即5个样本对应27个神经元的输出

|      |                |               |
| ---- | -------------- | ------------- |
| 维度说明 | 矩阵形状           | 结果形状          |
| 输入向量 | $5 \times 27$  |               |
| 权重矩阵 | $27 \times 27$ |               |
| 输出   |                | $5 \times 27$ |

*   输出中的元素 $(i, j)$ 表示第$i$个输入样本经过第$j$个神经元后的激活值

## 验证单个元素计算

*   矩阵乘法中的元素可以拆分为点积：

$$ \text{output}*{i,j} = \sum*{k=1}^{27} x\_{encoded}\[i,k] \cdot w\[k,j] $$

*   例如，第三个输入样本和第13个神经元对应的输出：

<!---->

    sum(x_encoded[2] * w[:, 12])

# 🎯 神经网路输出的概率分布解释

> “神经网络输出的27个数字需要被解释为下一字符的概率分布。”

## 输出的原始数值特点

*   由随机初始化的权重产生，包含正负值，没有规范化

*   不满足概率分布的两个关键条件：

    *   非负
    *   总和为1

## 概率分布的特性

|     |           |
| --- | --------- |
| 特性  | 描述        |
| 非负  | 概率值不能为负   |
| 归一化 | 概率总和必须等于1 |

*   直接输出的神经网络激活值不满足这两个条件

## 输出数值的解释方式：对数计数（Log Counts）

*   神经网络输出被视为“对数计数”，即对概率计数取对数后的值
*   要获得真实的概率，需要对输出进行指数映射：

$$ p\_i = e^{z\_i} $$

其中，$z\_i$是神经网络的输出（对数计数）

*   指数函数将所有实数映射到正数（$\mathbb{R} \to \mathbb{R}^+$）
*   负数输入指数后得到0到1之间的数值，正数输入指数后大于1

## 从对数计数到概率的过程

*   先对神经网络输出做指数运算得到非负数
*   再将这些非负数归一化，使其和为1（即概率）

这个过程通常用\*\*softmax函数（归一化指数函数）\*\*实现：

$$ p\_i = \frac{e^{z\_i}}{\sum\_{j=1}^{27} e^{z\_j}} $$

# 📊 输入编码与神经网络层示意表

|         |            |              |                  |
| ------- | ---------- | ------------ | ---------------- |
| 阶段      | 数据形状       | 数据类型         | 说明               |
| 输入整数    | $(5,)$     | 整数（Long）     | 字符索引             |
| 一热编码    | $(5, 27)$  | 整数（Long）     | 只有对应位置是1         |
| 类型转换    | $(5, 27)$  | 浮点数（float32） | 转为浮点数，满足神经网络输入要求 |
| 权重矩阵    | $(27, 27)$ | 浮点数          | 27个神经元，每个有27个权重  |
| 神经网络输出  | $(5, 27)$  | 浮点数          | 每个样本27个神经元输出     |
| 对数计数转概率 | $(5, 27)$  | 浮点数          | 使用指数和归一化得到概率     |

# 🔍 重点概念总结

|                      |                              |
| -------------------- | ---------------------------- |
| 概念                   | 解释                           |
| **一热编码**             | 用0和1构成的向量，表示类别型整数变量          |
| **权重初始化**            | 使用正态分布随机生成，均值0，标准差1的随机数      |
| **矩阵乘法**             | 批量计算神经元对多个输入的加权和             |
| **对数计数（log counts）** | 神经网络输出的实数，解释为计数的对数           |
| **概率分布**             | 非负且和为1的数值集合，通过softmax从对数计数导出 |

# 🧮 公式汇总

*   神经元输出（无偏置）：

$$ y = w \cdot x = \sum\_{i=1}^{27} w\_i x\_i $$

*   多神经元输出：

$$ Y = X W $$

其中，$X$是输入矩阵，$W$是权重矩阵，$Y$是输出矩阵。

*   对数计数转概率：

$$ p\_i = \frac{e^{z\_i}}{\sum\_{j=1}^{27} e^{z\_j}} $$

（$z\_i$为神经网络输出的第$i$个分量）

以上内容涵盖了整数输入编码、神经元构建与批量计算、以及神经网络输出概率解释等重要知识点，确保学生能够理解如何将离散输入转换为神经网络可处理的数值输入，并理解神经网络层输出的语义。

# 🔢 从logits到概率分布的神经网络输出

> logits（逻辑值）是神经网络输出的未归一化的对数计数，经过指数和归一化后转化为概率分布。

## 1. logits与计数的关系

*   神经网络的输出通常是一些实数，这些实数称为**logits**，可以视为“对数计数”（log counts）。
*   通过对logits进行**逐元素指数运算**，将负数变为小于1的正数，将正数变为大于1的数值，从而得到“伪计数”（fake counts）。
*   这些伪计数对应于原始计数的指数形式，保证数值非负，适合表示计数。

|              |                            |
| ------------ | -------------------------- |
| 变量含义         | 说明                         |
| logits ($z$) | 神经网络输出的对数计数（可正可负）          |
| 伪计数          | $e^{z}$，对logits指数运算后得到的非负数 |
| 计数矩阵 ($n$)   | 原始计数矩阵，每行对应下一字符的计数         |
| 概率矩阵 ($p$)   | 对计数矩阵行归一化后的概率分布            |

## 2. 计算概率的步骤

1.  网络输出logits $z$（形状为 $5 \times 27$，对应5个样本，每个样本27个字符类别）
2.  对logits进行指数运算得到伪计数：$c = e^{z}$
3.  对伪计数按行归一化，得到概率分布： $$p\_i = \frac{c\_i}{\sum\_j c\_j}$$
4.  每行的概率和为1，且所有概率均为正数。

## 3. softmax函数

*   softmax是将任意实数向量转换为概率分布的函数，具体步骤是先指数化再归一化。
*   公式： $$\text{softmax}(z\_i) = \frac{e^{z\_i}}{\sum\_j e^{z\_j}}$$
*   softmax使得神经网络输出变成概率，便于分类任务的处理。
*   该层是**可微分**的，方便进行梯度反向传播。

# 🧮 神经网络的前向传播（Forward Pass）

|               |                                                    |
| ------------- | -------------------------------------------------- |
| 步骤            | 详细说明                                               |
| 输入数据          | 将字符索引转换为**one-hot编码**，形状为 $5 \times 27$            |
| 线性变换          | 对one-hot编码乘以权重矩阵 $W$，得到logits $z$，形状 $5 \times 27$ |
| 计算softmax概率分布 | 对logits进行指数和归一化，得到输出概率分布 $p$                       |

*   输入为整数索引，经过one-hot编码后变成稀疏向量，只包含0和1。
*   权重矩阵$W$的大小为 $27 \times 27$（27个输入神经元，每个对应27个输出神经元）。
*   经过softmax后，输出每个样本预测下一字符的概率分布。

# 🔄 反向传播与损失函数

## 1. 负对数似然损失（Negative Log Likelihood Loss）

*   目标是使得神经网络给正确下一字符分配更高概率。
*   对每个样本，取正确标签对应的概率$p\_{true}$，计算负对数似然： $$\text{NLL} = -\log(p\_{true})$$
*   损失是所有样本NLL的平均值： $$\text{Loss} = \frac{1}{N} \sum\_{i=1}^N -\log(p\_{true}^{(i)})$$
*   负对数似然越大，说明网络对正确字符的预测越差。

## 2. 反向传播的可微分性

|         |            |
| ------- | ---------- |
| 操作      | 可微分性说明     |
| 矩阵乘法    | 可微分，梯度可计算  |
| 指数运算    | 可微分，链式法则应用 |
| 归一化（除法） | 可微分，梯度可计算  |
| 负对数运算   | 可微分，梯度可计算  |

*   因此，整个前向传播过程是可微分的，可以通过链式法则计算损失对权重的梯度。

# 🧩 具体示例分析

|      |      |               |         |            |
| ---- | ---- | ------------- | ------- | ---------- |
| 输入字符 | 输入索引 | 预测概率分布中正确字符概率 | 负对数似然损失 | 说明         |
| `.`  | 0    | 0.01          | 高       | 预测错误率高，损失大 |
| `e`  | 5    | 0.01          | 高       | 预测错误率高     |
| `m`  |      | 0.02          | 高       | 预测仍较差      |
| `m`  |      | 0.07          | 低       | 预测较好，损失较低  |
| `a`  |      | 0.01          | 高       | 预测较差       |

*   目前网络权重$W$随机初始化，预测不准确，损失较高。
*   通过调整$W$，可以降低平均负对数似然损失。

# ⚙️ 权重初始化与优化尝试

*   **随机初始化**权重$W$，尝试不同随机种子得到不同初始权重。
*   观察不同权重下的损失值：

|      |         |             |
| ---- | ------- | ----------- |
| 尝试次数 | 损失值     | 说明          |
| 1    | 3.76    | 初始随机权重，较高损失 |
| 2    | 3.37    | 更优权重，损失降低   |
| 3    | 4.5（示例） | 可能更差，损失升高   |

*   随机猜测权重并非优化神经网络的有效方法。
*   需要利用**梯度下降**基于损失函数计算梯度，迭代更新权重$W$，逐步降低损失。

# 🏗️ 梯度下降优化流程

|          |                                                                                              |
| -------- | -------------------------------------------------------------------------------------------- |
| 步骤       | 说明                                                                                           |
| 计算损失     | 计算当前权重$W$下的平均负对数似然损失                                                                         |
| 反向传播计算梯度 | 计算损失对$W$的梯度$\frac{\partial \text{Loss}}{\partial W}$                                         |
| 更新权重     | 按梯度方向调整权重：$W \leftarrow W - \eta \cdot \frac{\partial \text{Loss}}{\partial W}$ （$\eta$为学习率） |
| 重复迭代     | 多次重复以上步骤，直到损失收敛或达到设定的条件                                                                      |

*   优化方法与之前微积分梯度下降（micrograd）课程内容类似。
*   关键是保证所有运算可微分，支持梯度计算和反向传播。

# 📝 术语与概念总结

|              |                            |
| ------------ | -------------------------- |
| 术语           | 定义                         |
| logits (逻辑值) | 神经网络输出的实数，表示对数计数，未归一化      |
| softmax      | 将logits转换为概率分布的函数，先指数化再归一化 |
| 负对数似然损失      | 衡量概率预测与真实标签差异的损失函数，越小越好    |
| one-hot编码    | 将类别索引转换为只有一个位置为1，其余为0的向量表示 |
| 反向传播         | 计算损失相对于网络参数梯度的算法，用于参数优化    |

# 🧮 关键数学表达式

*   伪计数计算： $$c\_i = e^{z\_i}$$
*   概率归一化： $$p\_i = \frac{c\_i}{\sum\_j c\_j} = \frac{e^{z\_i}}{\sum\_j e^{z\_j}}$$
*   负对数似然损失： $$\text{Loss} = -\frac{1}{N} \sum\_{i=1}^N \log p\_{true}^{(i)}$$
*   权重更新公式： $$W \leftarrow W - \eta \cdot \frac{\partial \text{Loss}}{\partial W}$$

# 🎯 学习要点

*   神经网络输出的logits通过softmax转换为概率分布。
*   softmax保证输出满足概率要求（非负，和为1）。
*   负对数似然损失衡量预测概率与真实标签的一致程度。
*   所有计算步骤均为可微分操作，支持梯度反向传播。
*   通过优化权重矩阵，使损失最小化，提升模型预测准确度。

# 🧠 神经网络前向传播与反向传播详解

> 神经网络的训练过程主要包括前向传播（forward pass）、损失计算（loss calculation）、反向传播（backward pass）和参数更新（parameter update）。本节内容详细讲解了这些步骤，结合PyTorch实现和之前micrograd框架的对比。

## 1. 数据与输入格式

|        |      |                     |
| ------ | ---- | ------------------- |
| 变量     | 说明   | 细节                  |
| 输入样本数量 | 5    | 之前micrograd中有4个输入样本 |
| 输入维度   | 27   | 之前是3维，现扩展为27维向量     |
| 目标标签   | 整数序列 | 将标签转换为对应的向量索引       |

*   之前用的是多层感知机（Multi-Layer Perceptron，多层前馈神经网络）
*   现在用的是**单层线性层（linear layer）+ Softmax（归一化概率层）**

## 2. 前向传播与损失计算

### 2.1 前向传播

*   输入数据通过网络得到预测概率`probs`
*   形状为 $5 \times 27$，对应5个样本，每个样本有27个类别的预测概率

### 2.2 损失函数：负对数似然损失（Negative Log-Likelihood，NLL）

> 负对数似然损失用于分类任务，衡量模型预测概率与真实标签之间的差异。

*   目标为计算正确类别对应的概率的负对数平均

*   计算步骤：

    1.  从`probs`中提取正确类别的概率（根据标签索引）
    2.  取对数（log）
    3.  取平均（mean）
    4.  取负号，得到最终损失值

### 2.3 概率索引提取方式（PyTorch示例）

    row_indices = torch.arange(5)  # 生成0到4的序列，代表样本索引
    correct_class_probs = probs[row_indices, ys]  # ys为标签数组，直接索引对应概率
    loss = -torch.mean(torch.log(correct_class_probs))

*   这样避免了写成元组形式，大幅提升代码效率
*   计算得到的损失值示例：3.76

## 3. 反向传播

### 3.1 梯度清零

*   PyTorch中可以用`param.grad = None`替代`param.grad.zero_()`
*   `None`更高效，表示无梯度

### 3.2 启用梯度计算

*   张量需要设置`requires_grad=True`，否则不会计算梯度
*   这是PyTorch与micrograd的差异之一

### 3.3 反向传播过程

*   调用`loss.backward()`，PyTorch自动构建计算图（computational graph）
*   计算图记录所有运算及依赖关系
*   反向传播填充所有参数的梯度`param.grad`

### 3.4 梯度含义

|                |                |                                |
| -------------- | -------------- | ------------------------------ |
| 变量             | 形状             | 说明                             |
| 权重 $W$         | $27 \times 27$ | 网络参数矩阵                         |
| 梯度 $W\_{grad}$ | $27 \times 27$ | 每个元素表示对损失的梯度影响，正值表示正向增加损失，负值反之 |

> 例如，$W\_{0,0}$的梯度为正，表示稍微增加$W\_{0,0}$的值会使损失增大。

## 4. 参数更新：梯度下降

### 4.1 更新规则

$$W \leftarrow W - \eta \cdot W\_{grad}$$

*   其中$\eta$为学习率（learning rate），这里用负数乘以梯度，等同于负梯度方向更新

### 4.2 PyTorch代码示例

    learning_rate = 0.1
    W.data += -learning_rate * W.grad

*   更新后，前向传播的损失应该下降

### 4.3 迭代示例

|       |       |
| ----- | ----- |
| 迭代次数  | 损失值变化 |
| 初始    | 3.76  |
| 第一次更新 | 3.74  |
| 第二次更新 | 3.72  |

## 5. 批量训练与扩展

### 5.1 从小批量到大规模训练

*   最初只用5个二元组（bigrams）进行训练
*   后来扩展到22.8万个二元组，无需修改代码
*   训练过程和步骤完全相同，表现出算法的扩展性

### 5.2 学习率调节

|             |      |            |
| ----------- | ---- | ---------- |
| 学习率($\eta$) | 训练效果 | 备注         |
| 0.1         | 收敛较慢 | 适合小学习率     |
| 50          | 仍能收敛 | 简单模型中可用大步长 |

### 5.3 长时间迭代效果

*   100次迭代后损失降低到约2.47
*   继续迭代后趋近于2.45\~2.46

## 6. 理论与实践一致性

*   损失值在2.45左右是合理预期
*   之前统计计数方法得到的损失和梯度下降方法得到的结果相近
*   说明梯度下降方法成功模拟了概率分布的统计规律

# ⚙️ 重要概念与对比

|        |                          |                                  |
| ------ | ------------------------ | -------------------------------- |
| 概念     | micrograd实现              | PyTorch实现                        |
| 网络结构   | 多层感知机                    | 单层线性层 + Softmax                  |
| 损失函数   | 均方误差（Mean Squared Error） | 负对数似然损失（Negative Log Likelihood） |
| 梯度清零   | 手动设置为0                   | `param.grad = None`更高效           |
| 梯度计算启动 | 默认开启                     | 需要`requires_grad=True`           |
| 计算图构建  | 显式构建                     | 自动构建                             |
| 参数更新   | 手动迭代所有参数                 | 单参数张量直接更新                        |

# 📝 代码片段汇总

    # 索引正确类别概率
    row_indices = torch.arange(batch_size)
    correct_probs = probs[row_indices, ys]

    # 损失计算
    loss = -torch.mean(torch.log(correct_probs))

    # 梯度清零
    W.grad = None

    # 反向传播
    loss.backward()

    # 参数更新
    learning_rate = 0.1
    W.data += -learning_rate * W.grad

# 📌 关键要点总结

*   **前向传播**产生预测概率，计算负对数似然损失
*   **反向传播**自动计算梯度，依赖计算图
*   **梯度清零**保证每次反向传播梯度累积正确
*   **参数更新**沿梯度负方向调整权重，降低损失
*   代码结构简单且可扩展至大规模数据集
*   实验结果与理论预期高度一致，验证了梯度下降的有效性

# 🧠 大型语言模型中的Bigram模型与神经网络优化方法

## Bigram模型的显式方法与神经网络梯度优化方法

> Bigram模型是基于前一个字符预测下一个字符的简单语言模型。

*   **显式方法**：直接统计所有字符对（bigram）出现的频率，计算概率，存储在一个表格中。这种方法在只考虑一个前字符时非常简单高效，不需要梯度优化。
*   **神经网络梯度优化方法**：通过神经网络输出下一个字符的**logits（对数概率）**，使用**softmax（归一化函数）和负对数似然损失函数（negative log likelihood loss）**，利用梯度下降进行优化。

|      |                 |                        |
| ---- | --------------- | ---------------------- |
| 方法   | 优点              | 缺点                     |
| 显式统计 | 简单直接，准确         | 仅适用于非常简单的bigram模型，扩展性差 |
| 梯度优化 | 灵活，易于扩展复杂神经网络模型 | 需要迭代训练，计算成本较大          |

## 神经网络模型的输入表示与矩阵乘法机制

*   输入字符编码为**one-hot向量（一位有效向量）**，其中只有对应字符位置是1，其余为0。
*   神经网络的第一层权重矩阵$W$与one-hot向量相乘，实际上是从矩阵中“抽取”对应字符的**行向量**，即： $$ \text{logits} = W \cdot x $$ 其中$x$是one-hot向量，$W$是权重矩阵。

> 这与显式统计中使用矩阵查找概率的方式本质上是相同的，只不过$W$现在是经过训练得到的对数计数值矩阵（log counts），而非直接的计数。

## 平滑（Smoothing）与正则化（Regularization）

### 平滑的作用

*   在显式统计中，为避免某些bigram概率为零，采用**加假计数**（如拉普拉斯平滑），使概率分布更加均匀。
*   计数越大，概率越趋近均匀分布。

### 神经网络中的等价机制

*   初始化权重矩阵$W$所有元素为零时，logits全为零，经过softmax后概率均匀分布。
*   通过在损失函数中加入**正则化项**，鼓励$W$的值接近零，实现类似平滑效果。

### 正则化的数学表达

*   采用**L2正则化**，即权重平方和： $$ \text{regularization loss} = \lambda \cdot \text{mean}(W^2) $$ 其中$\lambda$为正则化强度系数。
*   总损失函数为： $$ \text{loss} = \text{negative log likelihood loss} + \lambda \cdot \text{mean}(W^2) $$
*   这一项相当于在优化过程中加入一个“弹簧力”或“重力”，将$W$推向零，防止过拟合。

## Bigram模型的不可扩展性与神经网络的可扩展性

|          |              |                  |
| -------- | ------------ | ---------------- |
| 特点       | Bigram显式方法   | 神经网络方法           |
| 输入历史考虑长度 | 只考虑一个前一个字符   | 可扩展到多个字符甚至长序列    |
| 存储需求     | 指数级增长，存储容量受限 | 通过权重共享与参数化，存储量可控 |
| 计算复杂度    | 低            | 高，但可通过硬件加速优化     |
| 灵活性      | 低            | 高，可接纳更复杂模型结构     |

## 采样（Sampling）过程

*   采样从初始字符开始，根据当前字符的概率分布采样下一个字符。

*   采样步骤：

    1.  将当前字符编码为one-hot向量$x$。
    2.  计算logits：$\text{logits} = W \cdot x$。
    3.  对logits进行指数运算并归一化，得到概率分布。
    4.  根据概率分布采样下一个字符索引。
    5.  重复该过程直到满足终止条件（如采样到终止符）。

*   由于显式统计模型和神经网络模型本质相同，采样结果一致。

## 关键概念总结表

|                         |                         |
| ----------------------- | ----------------------- |
| 关键概念                    | 说明                      |
| **Bigram模型**            | 使用前一个字符预测下一个字符的语言模型     |
| **One-hot向量**           | 只有一个位置为1，其余为0的向量，用于字符编码 |
| **Logits（对数概率）**        | 神经网络输出，softmax前的数值      |
| **Softmax函数**           | 将logits转换为概率分布          |
| **负对数似然损失**             | 用于衡量预测概率与真实标签之间差异的损失函数  |
| **正则化（Regularization）** | 通过增加权重平方和项防止过拟合，类似概率平滑  |
| **平滑（Smoothing）**       | 给概率分布加入假计数，避免零概率        |
| **采样（Sampling）**        | 根据概率分布生成字符序列的过程         |

## 未来展望：复杂神经网络架构的扩展

*   目前模型只考虑单个前字符，使用单层线性层计算logits。
*   未来将扩展为考虑多个前字符，输入更长序列。
*   复杂的神经网络架构（如循环神经网络RNN、卷积神经网络CNN、变换器Transformer）将被引入。
*   尽管前向传播变得复杂，**损失函数和优化机制保持不变**，仍使用负对数似然和梯度方法。
*   这使得模型具有更强的表达能力，能捕捉更长距离的上下文依赖。

# ⚙️ 代码逻辑示意

|             |                                                             |
| ----------- | ----------------------------------------------------------- |
| 步骤          | 说明                                                          |
| 1. 编码       | 输入字符索引转为one-hot向量                                           |
| 2. 计算logits | $ \text{logits} = W \cdot x $：将one-hot向量与权重矩阵相乘获取对应行的logits |
| 3. 归一化概率    | $p = \text{softmax}(\text{logits})$                         |
| 4. 采样       | 根据概率$p$随机选择下一个字符索引                                          |
| 5. 重复       | 用采样出的字符索引作为下一输入，循环采样直到终止                                    |
| 6. 训练       | 用负对数似然损失计算误差，结合正则化项，通过梯度下降更新$W$                             |

# ✨ 重要提示

*   **神经网络权重$W$与显式计数矩阵本质相同，差别在于训练方式：直接统计 vs 梯度优化。**
*   **正则化项控制模型复杂度，避免过拟合，等同于概率平滑。**
*   **显式bigram方法不适合长上下文，但神经网络方法易于扩展到复杂模型，如Transformer。**

# 📚 参考名词解释

> logits（对数概率）：神经网络输出的未经归一化的分数，用于计算概率。 softmax函数：将任意实数向量转化为概率分布的函数。 负对数似然损失：评估模型预测概率与真实标签差异的常用损失。 one-hot编码：用于表示类别的一种方式，只有对应位置为1，其余为0。 正则化：防止模型过拟合的技术，通常通过惩罚模型复杂度实现。
