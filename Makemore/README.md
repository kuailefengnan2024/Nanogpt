# makemore

makemore 接收一个文本文件作为输入，其中每一行被假定为一个训练样本，然后生成更多类似的样本。其底层是一个自回归的字符级语言模型，可以选择的模型范围很广，从二元组 (bigrams) 一直到 Transformer（与 GPT 中看到的一样）。例如，我们可以给它输入一个名字数据库，makemore 就能生成听起来像名字但又不存在的酷炫婴儿名字。或者，如果我们给它输入一个公司名称数据库，那么我们就可以为公司生成新的名称创意。或者我们也可以只给它输入有效的拼字游戏单词，然后生成类似英语的胡言乱语。

这并非一个拥有无数开关和旋钮的重量级库。它只是一个可供修改的文件，主要用于教育目的。[PyTorch](https://pytorch.org) 是唯一的要求。

当前的实现遵循了几篇关键论文：

- Bigram（一个字符通过计数查找表预测下一个字符）
- MLP，遵循 [Bengio 等人 2003 年的论文](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- CNN，遵循 [DeepMind WaveNet 2016 年的论文](https://arxiv.org/abs/1609.03499) （进行中...）
- RNN，遵循 [Mikolov 等人 2010 年的论文](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
- LSTM，遵循 [Graves 等人 2014 年的论文](https://arxiv.org/abs/1308.0850)
- GRU，遵循 [Kyunghyun Cho 等人 2014 年的论文](https://arxiv.org/abs/1409.1259)
- Transformer，遵循 [Vaswani 等人 2017 年的论文](https://arxiv.org/abs/1706.03762)

### 用法

以附带的 `names.txt` 数据集为例，它包含了从 [ssa.gov](https://www.ssa.gov/oact/babynames/) 获取的 2018 年最常见的 32K 个名字。看起来像这样：

```
emma
olivia
ava
isabella
sophia
charlotte
...
```

让我们将脚本指向它：

```bash
$ python makemore.py -i names.txt -o names
```

训练进度、日志和模型都将保存到工作目录 `names` 中。默认模型是一个仅有 200K 参数的超小型 transformer；还有更多可用的训练配置——请查看 argparse 参数并阅读代码。训练不需要任何特殊硬件，它可以在我的 Macbook Air 上运行，也可以在其他任何设备上运行，但如果你有 GPU，训练速度会更快。随着训练的进行，脚本会打印一些样本。但是，如果你想手动采样，可以使用 `--sample-only` 标志，例如，在另一个终端中执行：

```bash
$ python makemore.py -i names.txt -o names --sample-only
```

这将加载迄今为止最好的模型，并根据需要打印更多样本。以下是一些在当前默认设置下最终生成的独特婴儿名字（测试对数似然约为 1.92，尽管通过一些超参数调整可以实现更低的对数似然）：

```
dontell
khylum
camatena
aeriline
najlah
sherrith
ryel
irmi
taislee
mortaz
akarli
maxfelynn
biolett
zendy
laisa
halliliana
goralynn
brodynn
romima
chiyomin
loghlyn
melichae
mahmed
irot
helicha
besdy
ebokun
lucianno
```

玩得开心！


| `--type` 值 | 结构 | 备注 |
|-------------|------|------|
| `bigram`    | 简单查表  | 真·只看前一字符 |
| `mlp`       | Bengio-03 风格 n-gram MLP | 把前 *block_size* 个字符嵌入后串联 |
| `rnn`       | vanilla RNN | 自写 cell |
| `gru`       | 自写 GRU | 同上 |
| `bow`       | 词袋 + MLP | 伪注意力平均 |
| `transformer`(默认) | 小型 GPT-2 | 去掉 dropout 和预训练载入 |
共同点：

参数量都极小（默认几十万），CPU 也能跑得动。

全部自回归：喂 [<START>] 后生成字符直到采样到 <STOP>(0) 或凑够长度。




Logits 是一个三维浮点数张量 (B, T, V)，存储了模型对每个位置下一个 token 的原始预测分数。
让我们用一个具体的例子来描绘它实际的样子。假设：

*   **Batch Size (B) = 2** (我们一次处理两个句子)
*   **Sequence Length (T) = 3** (我们为每个句子的前三个词预测下一个词)
*   **Vocabulary Size (V) = 5** (我们的词汇表很小，只有5个词: "猫", "狗", "追", "跑", "<EOS>")
    *   "猫" -> 索引 0
    *   "狗" -> 索引 1
    *   "追" -> 索引 2
    *   "跑" -> 索引 3
    *   "<EOS>" (句子结束符) -> 索引 4

那么，这个 `Logits` 张量实际在内存中（或者概念上）看起来会像一个**嵌套的列表或数组**，包含 `B * T * V` 个浮点数：

```python
# Logits Tensor (形状: [2, 3, 5]) - Python列表形式表示

[  # <--- 批次维度 (B=2)

    # 第一个序列 (句子 1)
    [  # <--- 序列长度维度 (T=3)
        # 预测位置 1 (第一个词之后) 的 Logits
        # 对应词汇表 ["猫", "狗", "追", "跑", "<EOS>"] 的分数
        [1.5, -0.2, 0.8, -1.0, -2.5],  # 模型觉得 "猫" (1.5) 最可能，其次是 "追" (0.8)

        # 预测位置 2 (第二个词之后) 的 Logits
        [-0.1, 2.1, 0.5, 1.8, -0.9],  # 模型觉得 "狗" (2.1) 最可能，其次是 "跑" (1.8)

        # 预测位置 3 (第三个词之后) 的 Logits
        [-2.0, -1.5, 0.1, -0.5, 3.0]   # 模型觉得 "<EOS>" (3.0) 最可能
    ], # <--- 结束第一个序列

    # 第二个序列 (句子 2)
    [  # <--- 序列长度维度 (T=3)
        # 预测位置 1 (第一个词之后) 的 Logits
        [-0.8, 1.9, 1.2, 0.5, -1.1],  # 模型觉得 "狗" (1.9) 最可能，其次是 "追" (1.2)

        # 预测位置 2 (第二个词之后) 的 Logits
        [0.3, -0.6, 2.5, -0.1, -1.8],  # 模型觉得 "追" (2.5) 最可能

        # 预测位置 3 (第三个词之后) 的 Logits
        [-1.2, 0.9, -0.3, 2.8, 0.2]   # 模型觉得 "跑" (2.8) 最可能
    ]  # <--- 结束第二个序列

] # <--- 结束批次
```

**解读这个结构:**

1.  **最外层列表**：有 `B=2` 个元素，每个元素代表一个独立的输入序列（句子）。
2.  **中间层列表**：在每个序列内部，有 `T=3` 个元素（列表）。每个元素代表序列中一个特定位置的预测。`logits[i][j]` 就是第 `i` 个序列在第 `j` 个位置的预测向量。
3.  **最内层列表**：每个预测位置都对应一个长度为 `V=5` 的列表。这个列表包含了模型对词汇表中**每一个词**作为下一个词的原始打分（Logits）。`logits[i][j][k]` 就是第 `i` 个序列、第 `j` 个位置、对词汇表中第 `k` 个词的预测分数。

**总结:**

`Logits` 张量实际上是一个**三维的浮点数数组/网格**。你可以把它想象成：

*   一个包含 `B` 个 **二维表格** 的集合。
*   每个二维表格有 `T` 行，`V` 列。
*   表格中的每个单元格 `(t, v)` 存储了在序列的第 `t` 个位置预测词汇表中第 `v` 个词的分数。
*   

# =============================================================================


+---------------+      +----------------------+      +-----------------+
| [CharDataset] | ---->| [InfiniteDataLoader] | ---->| [Training Loop] |
+---------------+      +----------------------+      +--------+--------+
                                                               |
                                                               v
+-------------------------------------------------------------------------------------------------------------------------+
|                                                    Model Training Branches                                                |
+-------------------------------------------------------------------------------------------------------------------------+
      |                     |                      |                      |                      |
      v                     v                      v                      v                      v
+---------------+     +-----------+          +-----------+          +-----------+          +-----------+
| [Transformer] |     |   [BoW]   |          |   [RNN]   |          |   [MLP]   |          |  [Bigram] |
+---------------+     +-----------+          +-----------+          +-----------+          +-----------+
      |                     |                      |                      |                      |
      v                     v                      v                      v                      v
+---------------+     +------------+         +-----------+          +-------------+        +-------------+
|    [Block]    |     | [BoWBlock] |         | [RNNCell] |          | (MLP Int.)  |        | (Bigram Int)|
| (Contains...) |     +------------+         |  or       |          | (Emb,Concat,|        | (Emb Lookup)|
+---------------+           |                | [GRUCell] |          | Hid+Act...) |        +-------------+
      |                     v                +-----------+          +-------------+              |
      v               +-------------+              |                      |                      |
+-------------+       | [CausalBoW] |              |                      |                      |
| [CausalSelf |       +-------------+              |                      |                      |
|  Attention] |             |                      |                      |                      |
+-------------+             |                      |                      |                      |
      |                     |                      |                      |                      |
      | (Block Output)      | (Pooled Embed)       | (Hidden State)       | (Last Hidden)        | (Embedding)
      v                     v                      v                      v                      v
+-------------+     +-------------+        +-------------+        +-------------+        +-------------+
| [Linear     |     | [Linear     |        | [Linear     |        | [Linear     |        | [Linear     |
| (Output)]   |     | (Output)]   |        | (Output)]   |        | (Output)]   |        | (Output)]   |
+-------------+     +-------------+        +-------------+        +-------------+        +-------------+
      |                     |                      |                      |                      |
      v                     v                      v                      v                      v
+-------------+     +-------------+        +-------------+        +-------------+        +-------------+
|   Logits    |     |   Logits    |        |   Logits    |        |   Logits    |        |   Logits    |
+-------------+     +-------------+        +-------------+        +-------------+        +-------------+
      |                     |                      |                      |                      |
      +---------------------+----------------------+----------------------+----------------------+
                                             |
                                             v
                                     +-----------------+
                                     | Loss Calculation|
                                     | (in Training L.)|
                                     +-----------------+
