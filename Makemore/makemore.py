"""
ä½ ç»™è¿™ä¸ªè„šæœ¬ä¸€äº›å•è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼Œå®ƒä¼šç”Ÿæˆæ›´å¤šç±»ä¼¼çš„ä¸œè¥¿ã€‚
ä½¿ç”¨æœ€å…ˆè¿›çš„ Transformer AI æŠ€æœ¯
è¿™æ®µä»£ç æ—¨åœ¨éå¸¸æ˜“äºä¿®æ”¹ã€‚æ ¹æ®ä½ çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

ä¸ minGPT çš„åŒºåˆ«ï¼š
- æˆ‘åˆ é™¤äº†ä» GPT2 æƒé‡åˆå§‹åŒ–çš„ from_pretrained å‡½æ•°
- æˆ‘åˆ é™¤äº† dropout å±‚ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿™é‡Œè®­ç»ƒçš„æ¨¡å‹å¾ˆå°ï¼Œ
  åœ¨è¿™ä¸ªé˜¶æ®µå’Œè¿™ä¸ªè§„æ¨¡ä¸‹æ²¡æœ‰å¿…è¦ç†è§£å®ƒã€‚
- æˆ‘åˆ é™¤äº†æƒé‡è¡°å‡ä»¥åŠå›´ç»•å“ªäº›å‚æ•°è¿›è¡Œæƒé‡è¡°å‡çš„æ‰€æœ‰å¤æ‚æ€§ã€‚
  æˆ‘ç›¸ä¿¡è¿™åœ¨æˆ‘ä»¬è¿™é‡Œçš„æ“ä½œè§„æ¨¡ä¸‹ä¸ä¼šäº§ç”Ÿå·¨å¤§å·®å¼‚ã€‚
"""

# =============================================================================
# ğŸ”§ é…ç½®è®¾ç½®åŒºåŸŸ - åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„è®­ç»ƒè®¾ç½®
# =============================================================================

# ğŸ“ æ–‡ä»¶å’Œè¾“å‡ºè®¾ç½®
INPUT_FILE = 'names.txt'              # è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªå•è¯ï¼‰
WORK_DIR = 'out'                      # è¾“å‡ºç›®å½•
RESUME = False                        # æ˜¯å¦ä»ç°æœ‰æ¨¡å‹æ¢å¤è®­ç»ƒ
SAMPLE_ONLY = False                   # æ˜¯å¦åªé‡‡æ ·ä¸è®­ç»ƒ

# ğŸ¤– æ¨¡å‹è®¾ç½® - é€‰æ‹©æ¨¡å‹ç±»å‹
MODEL_TYPE = 'transformer'            # æ¨¡å‹ç±»å‹: transformer|bigram|mlp|rnn|gru|bow

# ğŸ“Š å„æ¨¡å‹å‚æ•°è¯´æ˜å’Œè®¾ç½®
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ æ¨¡å‹        â”‚ n_layer â”‚ n_head  â”‚ n_embd  â”‚ n_embd2  â”‚ è¯´æ˜     â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ transformer â”‚    âœ“    â”‚    âœ“    â”‚    âœ“    â”‚    âœ“     â”‚ å…¨éƒ¨éœ€è¦  â”‚
# â”‚ bigram      â”‚    âœ—    â”‚    âœ—    â”‚    âœ—    â”‚    âœ—     â”‚ ä»…éœ€è¯æ±‡  â”‚
# â”‚ mlp         â”‚    âœ—    â”‚    âœ—    â”‚    âœ“    â”‚    âœ“     â”‚ åµŒå…¥ç»´åº¦  â”‚
# â”‚ rnn/gru     â”‚    âœ—    â”‚    âœ—    â”‚    âœ“    â”‚    âœ“     â”‚ åµŒå…¥ç»´åº¦  â”‚
# â”‚ bow         â”‚    âœ—    â”‚    âœ—    â”‚    âœ“    â”‚    âœ“     â”‚ åµŒå…¥ç»´åº¦  â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# ğŸ—ï¸ æ¨¡å‹æ¶æ„å‚æ•°ï¼ˆæ ¹æ®ä¸Šè¡¨ï¼ŒæŸäº›æ¨¡å‹ä¼šå¿½ç•¥ä¸éœ€è¦çš„å‚æ•°ï¼‰
N_LAYER = 4                          # Transformerå±‚æ•°ï¼ˆä»…transformerä½¿ç”¨ï¼‰
N_HEAD = 4                           # æ³¨æ„åŠ›å¤´æ•°ï¼ˆä»…transformerä½¿ç”¨ï¼‰
N_EMBD = 64                          # ä¸»è¦åµŒå…¥ç»´åº¦ï¼ˆtransformer|mlp|rnn|gru|bowï¼‰
N_EMBD2 = 64                         # è¾…åŠ©åµŒå…¥ç»´åº¦ï¼ˆtransformer|mlp|rnn|gru|bowï¼‰

# ğŸƒ è®­ç»ƒè®¾ç½®
BATCH_SIZE = 32                      # æ‰¹å¤§å°
LEARNING_RATE = 5e-4                 # å­¦ä¹ ç‡
WEIGHT_DECAY = 0.01                  # æƒé‡è¡°å‡
MAX_STEPS = -1                       # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆ-1ä¸ºæ— é™ï¼‰

# ğŸ’» ç³»ç»Ÿè®¾ç½®
DEVICE = 'cuda'                       # è®¡ç®—è®¾å¤‡: cpu|cuda|cuda:0|mps
SEED = 3407                          # éšæœºç§å­
NUM_WORKERS = 4                      # æ•°æ®åŠ è½½çº¿ç¨‹æ•°

# ğŸ² é‡‡æ ·è®¾ç½®
TOP_K = -1                           # Top-Ké‡‡æ ·ï¼ˆ-1ä¸ºä¸ä½¿ç”¨ï¼‰

# ğŸ›ï¸ å¿«é€Ÿé…ç½®é¢„è®¾ï¼ˆå–æ¶ˆæ³¨é‡Šæ¥ä½¿ç”¨ï¼‰
# å¦‚æœä½ æƒ³å¿«é€Ÿåˆ‡æ¢æ¨¡å‹é…ç½®ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„é…ç½®ç»„åˆï¼š

# # Bigramæ¨¡å‹ï¼ˆæœ€ç®€å•ï¼‰
# MODEL_TYPE = 'bigram'
# BATCH_SIZE = 64
# LEARNING_RATE = 1e-3
# MAX_STEPS = 1000

# # MLPæ¨¡å‹ï¼ˆç»å…¸ï¼‰
# MODEL_TYPE = 'mlp'
# N_EMBD = 128
# N_EMBD2 = 128
# BATCH_SIZE = 32
# MAX_STEPS = 5000

# # RNNæ¨¡å‹ï¼ˆå¾ªç¯ï¼‰
# MODEL_TYPE = 'rnn'
# N_EMBD = 128
# N_EMBD2 = 128
# LEARNING_RATE = 1e-3
# MAX_STEPS = 10000

# # Transformeræ¨¡å‹ï¼ˆæœ€å¼ºï¼‰
# MODEL_TYPE = 'transformer'
# N_LAYER = 6
# N_HEAD = 8
# N_EMBD = 128
# N_EMBD2 = 128
# BATCH_SIZE = 32
# MAX_STEPS = 10000

# =============================================================================



import os
import sys
import time
import math
from dataclasses import dataclass
from typing import List

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader
from torch.utils.tensorboard import SummaryWriter

# -----------------------------------------------------------------------------

@dataclass
class ModelConfig:
    block_size: int = None # è¾“å…¥æ•´æ•°åºåˆ—çš„é•¿åº¦
    vocab_size: int = None # è¾“å…¥æ•´æ•°çš„èŒƒå›´åœ¨ [0 .. vocab_size -1] ä¹‹é—´
    # ä¸‹é¢çš„å‚æ•°ä»¥ç•¥å¾®ä¸åŒçš„æ–¹å¼æ§åˆ¶æ¯ä¸ªæ¨¡å‹çš„å¤§å°
    n_layer: int = 4
    n_embd: int = 64
    n_embd2: int = 64
    n_head: int = 4

# -----------------------------------------------------------------------------
# Transformer è¯­è¨€æ¨¡å‹ï¼ˆä¸ GPT-2 ä¸­ä½¿ç”¨çš„å®Œå…¨ç›¸åŒï¼‰

class NewGELU(nn.Module):
    """
    å½“å‰ Google BERT ä»“åº“ä¸­ GELU æ¿€æ´»å‡½æ•°çš„å®ç°ï¼ˆä¸ OpenAI GPT ç›¸åŒï¼‰ã€‚
    å‚è€ƒï¼šé«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒ (GELU) è®ºæ–‡ï¼šhttps://arxiv.org/abs/1606.08415
    """
    def forward(self, x):
        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))

class CausalSelfAttention(nn.Module):
    """
    ä¸€ä¸ªæ™®é€šçš„å¤šå¤´æ©ç è‡ªæ³¨æ„åŠ›å±‚ï¼Œæœ«å°¾å¸¦æœ‰ä¸€ä¸ªæŠ•å½±ã€‚
    å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ torch.nn.MultiheadAttentionï¼Œä½†æˆ‘åœ¨è¿™é‡ŒåŒ…å«äº†ä¸€ä¸ª
    æ˜¾å¼çš„å®ç°ï¼Œä»¥è¡¨æ˜è¿™é‡Œæ²¡æœ‰ä»€ä¹ˆå¤ªå¯æ€•çš„ä¸œè¥¿ã€‚
    """

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # æ‰€æœ‰å¤´çš„é”®ã€æŸ¥è¯¢ã€å€¼æŠ•å½±ï¼Œä½†åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        # è¾“å‡ºæŠ•å½±
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        # å› æœæ©ç ï¼Œç¡®ä¿æ³¨æ„åŠ›åªåº”ç”¨äºè¾“å…¥åºåˆ—çš„å·¦ä¾§
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(1, 1, config.block_size, config.block_size))
        self.n_head = config.n_head
        self.n_embd = config.n_embd

    def forward(self, x):
        B, T, C = x.size() # æ‰¹å¤§å°ã€åºåˆ—é•¿åº¦ã€åµŒå…¥ç»´åº¦ (n_embd)

        # è®¡ç®—æ‰¹æ¬¡ä¸­æ‰€æœ‰å¤´çš„æŸ¥è¯¢ã€é”®ã€å€¼ï¼Œå¹¶å°†å¤´ç§»åŠ¨åˆ°æ‰¹æ¬¡ç»´åº¦
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # å› æœè‡ªæ³¨æ„åŠ›ï¼›è‡ªæ³¨æ„åŠ›ï¼š(B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # é‡æ–°ç»„åˆæ‰€æœ‰å¤´çš„è¾“å‡º

        # è¾“å‡ºæŠ•å½±
        y = self.c_proj(y)
        return y

class Block(nn.Module):
    """ ä¸€ä¸ªä¸èµ·çœ¼çš„ Transformer å— """

    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = nn.ModuleDict(dict(
            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),
            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),
            act     = NewGELU(),
        ))
        m = self.mlp
        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP å‰å‘ä¼ æ’­

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlpf(self.ln_2(x))
        return x

class Transformer(nn.Module):
    """ Transformer è¯­è¨€æ¨¡å‹ï¼Œä¸ GPT-2 ä¸­çœ‹åˆ°çš„ä¸€æ · """

    def __init__(self, config):
        super().__init__()
        self.block_size = config.block_size

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = nn.LayerNorm(config.n_embd),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # æŠ¥å‘Šå‚æ•°æ•°é‡ï¼ˆæ³¨æ„æˆ‘ä»¬ä¸è®¡ç®— lm_head ä¸­çš„è§£ç å™¨å‚æ•°ï¼‰
        n_params = sum(p.numel() for p in self.transformer.parameters())
        print("number of parameters: %.2fM" % (n_params/1e6,))

    def get_block_size(self):
        return self.block_size

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        assert t <= self.block_size, f"Cannot forward sequence of length {t}, block size is only {self.block_size}"
        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # å½¢çŠ¶ (1, t)

        # å‰å‘ä¼ æ’­ GPT æ¨¡å‹æœ¬èº«
        tok_emb = self.transformer.wte(idx) # token åµŒå…¥ï¼Œå½¢çŠ¶ (b, t, n_embd)
        pos_emb = self.transformer.wpe(pos) # ä½ç½®åµŒå…¥ï¼Œå½¢çŠ¶ (1, t, n_embd)
        x = tok_emb + pos_emb
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)
        logits = self.lm_head(x)

        # å¦‚æœç»™å®šäº†ç›®æ ‡ï¼Œåˆ™è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

        return logits, loss

# -----------------------------------------------------------------------------
# è¯è¢‹ (BoW) è¯­è¨€æ¨¡å‹

class CausalBoW(nn.Module):
    """
    å› æœè¯è¢‹ã€‚å¯¹å‰é¢çš„å…ƒç´ è¿›è¡Œå¹³å‡ï¼Œçœ‹èµ·æ¥å¾ˆåƒ
    ä½ åœ¨ Transformer ä¸­æ‰¾åˆ°çš„ CausalAttention æ¨¡å—ï¼ŒåŸå› ä¸æ˜ ;)
    """
    def __init__(self, config):
        super().__init__()

        # ç”¨äºå±è”½å‘é‡å¹¶ä¿ç•™è‡ªå›å½’å±æ€§
        self.block_size = config.block_size
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                            .view(1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # æ‰¹å¤§å°ã€åºåˆ—é•¿åº¦ã€n_embd

        # å¯¹æ‰€æœ‰å‰é¢çš„ token ç‰¹å¾è¿›è¡ŒåŠ æƒå¹³å‡
        att = torch.zeros((B, T, T), device=x.device)
        att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        y = att @ x # (B, T, T) x (B, T, C) -> (B, T, C)

        return y

class BoWBlock(nn.Module):
    """ æ”¶é›† BoW ç‰¹å¾å¹¶æ·»åŠ ä¸€ä¸ª MLP """

    def __init__(self, config):
        super().__init__()

        # å› æœ BoW æ¨¡å—
        self.cbow = CausalBoW(config)
        # MLP ç»„è£…å™¨
        self.mlp = nn.ModuleDict(dict(
            c_fc    = nn.Linear(config.n_embd, config.n_embd2),
            c_proj  = nn.Linear(config.n_embd2, config.n_embd),
        ))
        m = self.mlp
        self.mlpf = lambda x: m.c_proj(F.tanh(m.c_fc(x))) # MLP å‰å‘ä¼ æ’­

    def forward(self, x):
        x = x + self.cbow(x)
        x = x + self.mlpf(x)
        return x

class BoW(nn.Module):
    """
    è·å–ä¹‹å‰çš„ block_size ä¸ª tokenï¼Œä½¿ç”¨æŸ¥æ‰¾è¡¨å¯¹å…¶è¿›è¡Œç¼–ç ï¼Œ
    ä¹Ÿä½¿ç”¨æŸ¥æ‰¾è¡¨å¯¹å…¶ä½ç½®è¿›è¡Œç¼–ç ï¼Œç„¶åå°†æ‰€æœ‰è¿™äº›åµŒå…¥å¹³å‡èµ·æ¥ï¼Œ
    å¹¶ä½¿ç”¨å®ƒæ¥é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
    """

    def __init__(self, config):
        super().__init__()
        self.block_size = config.block_size
        self.vocab_size = config.vocab_size
        # token åµŒå…¥
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        # ä½ç½®åµŒå…¥
        self.wpe = nn.Embedding(config.block_size, config.n_embd)
        # ä¸Šä¸‹æ–‡å—
        self.context_block = BoWBlock(config)
        # è¯­è¨€æ¨¡å‹å¤´è§£ç å™¨å±‚
        self.lm_head = nn.Linear(config.n_embd, self.vocab_size)

    def get_block_size(self):
        return self.block_size

    def forward(self, idx, targets=None):

        device = idx.device
        b, t = idx.size()
        assert t <= self.block_size, f"Cannot forward sequence of length {t}, block size is only {self.block_size}"
        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # å½¢çŠ¶ (1, t)

        # å‰å‘ä¼ æ’­ token å’Œä½ç½®åµŒå…¥å±‚
        tok_emb = self.wte(idx) # token åµŒå…¥ï¼Œå½¢çŠ¶ (b, t, n_embd)
        pos_emb = self.wpe(pos) # ä½ç½®åµŒå…¥ï¼Œå½¢çŠ¶ (1, t, n_embd)
        # ç›¸åŠ å¹¶é€šè¿‡è§£ç å™¨ MLP
        x = tok_emb + pos_emb
        # è¿è¡Œè¯è¢‹ä¸Šä¸‹æ–‡æ¨¡å—
        x = self.context_block(x)
        # è§£ç ä¸ºä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡
        logits = self.lm_head(x)

        # å¦‚æœç»™å®šäº†ç›®æ ‡ï¼Œåˆ™è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

        return logits, loss

# -----------------------------------------------------------------------------
"""
å¾ªç¯ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼šæ™®é€š RNN å¾ªç¯æˆ– GRUã€‚
æ²¡æœ‰å®ç° LSTMï¼Œå› ä¸ºå®ƒçš„ API æœ‰ç‚¹çƒ¦äººï¼Œå› ä¸ºå®ƒæ—¢æœ‰éšè—çŠ¶æ€åˆæœ‰å•å…ƒçŠ¶æ€ï¼Œ
ä½†å®ƒä¸ GRU éå¸¸ç›¸ä¼¼ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­æ•ˆæœä¸€æ ·å¥½ã€‚
"""

class RNNCell(nn.Module):
    """
    'Cell' çš„å·¥ä½œæ˜¯ï¼š
    è·å–å½“å‰æ—¶é—´æ­¥ x_{t} çš„è¾“å…¥å’Œä¸Šä¸€ä¸ªæ—¶é—´æ­¥ h_{t-1} çš„éšè—çŠ¶æ€ï¼Œ
    å¹¶è¿”å›å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ h_{t}ã€‚
    """
    def __init__(self, config):
        super().__init__()
        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)

    def forward(self, xt, hprev):
        xh = torch.cat([xt, hprev], dim=1)
        ht = F.tanh(self.xh_to_h(xh))
        return ht

class GRUCell(nn.Module):
    """
    ä¸ RNN cell çš„å·¥ä½œç›¸åŒï¼Œä½†å¾ªç¯å…¬å¼æ›´å¤æ‚ä¸€äº›ï¼Œ
    è¿™ä½¿å¾— GRU æ›´å…·è¡¨ç°åŠ›ä¸”æ›´æ˜“äºä¼˜åŒ–ã€‚
    """
    def __init__(self, config):
        super().__init__()
        # è¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨ã€å€™é€‰é—¨
        self.xh_to_z = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)
        self.xh_to_r = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)
        self.xh_to_hbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)

    def forward(self, xt, hprev):
        # é¦–å…ˆä½¿ç”¨é‡ç½®é—¨å°†éšè—çŠ¶æ€çš„æŸäº›é€šé“ç½®é›¶
        xh = torch.cat([xt, hprev], dim=1)
        r = F.sigmoid(self.xh_to_r(xh))
        hprev_reset = r * hprev
        # è®¡ç®—å€™é€‰çš„æ–°éšè—çŠ¶æ€ hbar
        xhr = torch.cat([xt, hprev_reset], dim=1)
        hbar = F.tanh(self.xh_to_hbar(xhr))
        # è®¡ç®—æ›´æ–°é—¨ï¼Œç¡®å®šæ¯ä¸ªé€šé“æ˜¯å¦åº”è¯¥æ›´æ–°
        z = F.sigmoid(self.xh_to_z(xh))
        # æ··åˆå…ˆå‰çš„éšè—çŠ¶æ€å’Œæ–°çš„å€™é€‰éšè—çŠ¶æ€
        ht = (1 - z) * hprev + z * hbar
        return ht

class RNN(nn.Module):

    def __init__(self, config, cell_type):
        super().__init__()
        self.block_size = config.block_size
        self.vocab_size = config.vocab_size
        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # èµ·å§‹éšè—çŠ¶æ€
        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token åµŒå…¥è¡¨
        if cell_type == 'rnn':
            self.cell = RNNCell(config)
        elif cell_type == 'gru':
            self.cell = GRUCell(config)
        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)

    def get_block_size(self):
        return self.block_size

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()

        # ä¸ºäº†æ•ˆç‡ï¼Œä¸€æ¬¡æ€§åµŒå…¥æ‰€æœ‰æ•´æ•°
        emb = self.wte(idx) # (b, t, n_embd)

        # æŒ‰é¡ºåºè¿­ä»£è¾“å…¥å¹¶æ›´æ–°æ¯ä¸ªæ—¶é—´æ­¥çš„ RNN çŠ¶æ€
        hprev = self.start.expand((b, -1)) # æ‰©å±•æ‰¹æ¬¡ç»´åº¦
        hiddens = []
        for i in range(t):
            xt = emb[:, i, :] # (b, n_embd)
            ht = self.cell(xt, hprev) # (b, n_embd2)
            hprev = ht
            hiddens.append(ht)

        # è§£ç è¾“å‡º
        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)
        logits = self.lm_head(hidden)

        # å¦‚æœç»™å®šäº†ç›®æ ‡ï¼Œåˆ™è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

        return logits, loss

# -----------------------------------------------------------------------------
# MLP è¯­è¨€æ¨¡å‹

class MLP(nn.Module):
    """
    è·å–ä¹‹å‰çš„ block_size ä¸ª tokenï¼Œä½¿ç”¨æŸ¥æ‰¾è¡¨å¯¹å…¶è¿›è¡Œç¼–ç ï¼Œ
    è¿æ¥å‘é‡å¹¶é€šè¿‡ MLP é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚

    å‚è€ƒï¼š
    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
    """

    def __init__(self, config):
        super().__init__()
        self.block_size = config.block_size
        self.vocab_size = config.vocab_size
        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token åµŒå…¥è¡¨
        # ä¸Šé¢ä¸€è¡Œä¸­çš„ +1 ç”¨äºç‰¹æ®Šçš„ <BLANK> tokenï¼Œå¦‚æœåœ¨è¾“å…¥åºåˆ—å¼€å§‹ä¹‹å‰ç¼–ç  tokenï¼Œåˆ™æ’å…¥è¯¥ token
        self.mlp = nn.Sequential(
            nn.Linear(self.block_size * config.n_embd, config.n_embd2),
            nn.Tanh(),
            nn.Linear(config.n_embd2, self.vocab_size)
        )

    def get_block_size(self):
        return self.block_size

    def forward(self, idx, targets=None):

        # æ”¶é›†å‰ 3 ä¸ªè¯çš„è¯åµŒå…¥
        embs = []
        for k in range(self.block_size):
            tok_emb = self.wte(idx) # token åµŒå…¥ï¼Œå½¢çŠ¶ (b, t, n_embd)
            idx = torch.roll(idx, 1, 1)
            idx[:, 0] = self.vocab_size # ç‰¹æ®Šçš„ <BLANK> token
            embs.append(tok_emb)

        # å°†æ‰€æœ‰åµŒå…¥è¿æ¥åœ¨ä¸€èµ·å¹¶é€šè¿‡ MLP
        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)
        logits = self.mlp(x)

        # å¦‚æœç»™å®šäº†ç›®æ ‡ï¼Œåˆ™è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

        return logits, loss

# -----------------------------------------------------------------------------
# Bigram è¯­è¨€æ¨¡å‹

class Bigram(nn.Module):
    """
    Bigram è¯­è¨€æ¨¡å‹ 'ç¥ç»ç½‘ç»œ'ï¼Œä»…ä»…æ˜¯ä¸€ä¸ªæŸ¥æ‰¾è¡¨ï¼Œ
    æ ¹æ®å‰ä¸€ä¸ªå­—ç¬¦ç»™å‡ºä¸‹ä¸€ä¸ªå­—ç¬¦çš„ logitsã€‚
    """

    def __init__(self, config):
        super().__init__()
        n = config.vocab_size
        self.logits = nn.Parameter(torch.zeros((n, n)))

    def get_block_size(self):
        return 1 # è¿™ä¸ªæ¨¡å‹åªéœ€è¦ä¸€ä¸ªå‰é©±å­—ç¬¦æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª

    def forward(self, idx, targets=None):

         # 'å‰å‘ä¼ æ’­'ï¼Œå“ˆå“ˆ
        logits = self.logits[idx]

        # å¦‚æœç»™å®šäº†ç›®æ ‡ï¼Œåˆ™è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

        return logits, loss

# -----------------------------------------------------------------------------
# ç”¨äºè¯„ä¼°å’Œä»æ¨¡å‹ä¸­é‡‡æ ·çš„è¾…åŠ©å‡½æ•°

@torch.no_grad()
def generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):
    """
    è·å–ä¸€ä¸ªæ¡ä»¶åºåˆ—ç´¢å¼• idx (å½¢çŠ¶ä¸º (b, t) çš„ LongTensor)ï¼Œå¹¶å®Œæˆåºåˆ— max_new_tokens æ¬¡ï¼Œ
    æ¯æ¬¡å°†é¢„æµ‹åé¦ˆç»™æ¨¡å‹ã€‚
    æœ€å¯èƒ½çš„æƒ…å†µæ˜¯ï¼Œä½ éœ€è¦ç¡®ä¿ä¸ºæ­¤æ“ä½œå¤„äº model.eval() æ¨¡å¼ã€‚
    """
    block_size = model.get_block_size()
    for _ in range(max_new_tokens):
        # å¦‚æœåºåˆ—ä¸Šä¸‹æ–‡å¤ªé•¿ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨ block_size å¤„æˆªæ–­å®ƒ
        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]
        # å‰å‘ä¼ æ’­æ¨¡å‹ä»¥è·å–åºåˆ—ä¸­ç´¢å¼•çš„ logits
        logits, _ = model(idx_cond)
        # è·å–æœ€åä¸€æ­¥çš„ logits å¹¶æŒ‰æœŸæœ›çš„æ¸©åº¦ç¼©æ”¾
        logits = logits[:, -1, :] / temperature
        # å¯é€‰åœ°å°† logits è£å‰ªä¸ºä»…å‰ k ä¸ªé€‰é¡¹
        if top_k is not None:
            v, _ = torch.topk(logits, top_k)
            logits[logits < v[:, [-1]]] = -float('Inf')
        # åº”ç”¨ softmax å°† logits è½¬æ¢ä¸ºï¼ˆå½’ä¸€åŒ–çš„ï¼‰æ¦‚ç‡
        probs = F.softmax(logits, dim=-1)
        # è¦ä¹ˆä»åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œè¦ä¹ˆå–æœ€å¯èƒ½çš„å…ƒç´ 
        if do_sample:
            idx_next = torch.multinomial(probs, num_samples=1)
        else:
            _, idx_next = torch.topk(probs, k=1, dim=-1)
        # å°†é‡‡æ ·åˆ°çš„ç´¢å¼•é™„åŠ åˆ°è¿è¡Œåºåˆ—å¹¶ç»§ç»­
        idx = torch.cat((idx, idx_next), dim=1)

    return idx

def print_samples(num=10):
    """ ä»æ¨¡å‹ä¸­é‡‡æ ·å¹¶æ¼‚äº®åœ°æ‰“å°è§£ç åçš„æ ·æœ¬ """
    X_init = torch.zeros(num, 1, dtype=torch.long).to(args.device)
    top_k = args.top_k if args.top_k != -1 else None
    steps = train_dataset.get_output_length() - 1 # -1 æ˜¯å› ä¸ºæˆ‘ä»¬å·²ç»ä»¥ <START> token (ç´¢å¼• 0) å¼€å§‹äº†
    X_samp = generate(model, X_init, steps, top_k=top_k, do_sample=True).to('cpu')
    train_samples, test_samples, new_samples = [], [], []
    for i in range(X_samp.size(0)):
        # è·å–é‡‡æ ·æ•´æ•°çš„ç¬¬ i è¡Œï¼Œä½œä¸º python åˆ—è¡¨
        row = X_samp[i, 1:].tolist() # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦è£å‰ªæ‰ç¬¬ä¸€ä¸ª <START> token
        # token 0 æ˜¯ <STOP> tokenï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨æ­¤å¤„è£å‰ªè¾“å‡ºåºåˆ—
        crop_index = row.index(0) if 0 in row else len(row)
        row = row[:crop_index]
        word_samp = train_dataset.decode(row)
        # åˆ†åˆ«è·Ÿè¸ªæˆ‘ä»¬è§è¿‡å’Œæ²¡è§è¿‡çš„æ ·æœ¬
        if train_dataset.contains(word_samp):
            train_samples.append(word_samp)
        elif test_dataset.contains(word_samp):
            test_samples.append(word_samp)
        else:
            new_samples.append(word_samp)
    print('-'*80)
    for lst, desc in [(train_samples, 'åœ¨è®­ç»ƒé›†ä¸­'), (test_samples, 'åœ¨æµ‹è¯•é›†ä¸­'), (new_samples, 'æ–°çš„')]:
        print(f"{len(lst)} ä¸ªæ ·æœ¬ {desc}:")
        for word in lst:
            print(word)
    print('-'*80)

@torch.inference_mode()
def evaluate(model, dataset, batch_size=50, max_batches=None):
    model.eval()
    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)
    losses = []
    for i, batch in enumerate(loader):
        batch = [t.to(args.device) for t in batch]
        X, Y = batch
        logits, loss = model(X, Y)
        losses.append(loss.item())
        if max_batches is not None and i >= max_batches:
            break
    mean_loss = torch.tensor(losses).mean().item()
    model.train() # å°†æ¨¡å‹é‡ç½®å›è®­ç»ƒæ¨¡å¼
    return mean_loss

# -----------------------------------------------------------------------------
# ç”¨äºåˆ›å»ºå‘å‡ºå•è¯çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†çš„è¾…åŠ©å‡½æ•°

class CharDataset(Dataset):

    def __init__(self, words, chars, max_word_length):
        self.words = words
        self.chars = chars
        self.max_word_length = max_word_length
        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}
        self.itos = {i:s for s,i in self.stoi.items()} # åå‘æ˜ å°„

    def __len__(self):
        return len(self.words)

    def contains(self, word):
        return word in self.words

    def get_vocab_size(self):
        return len(self.chars) + 1 # æ‰€æœ‰å¯èƒ½çš„å­—ç¬¦å’Œç‰¹æ®Šçš„ 0 token

    def get_output_length(self):
        return self.max_word_length + 1 # <START> token åè·Ÿå•è¯

    def encode(self, word):
        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)
        return ix

    def decode(self, ix):
        word = ''.join(self.itos[i] for i in ix)
        return word

    def __getitem__(self, idx):
        word = self.words[idx]
        ix = self.encode(word)
        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)
        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)
        x[1:1+len(ix)] = ix
        y[:len(ix)] = ix
        y[len(ix)+1:] = -1 # ç´¢å¼• -1 å°†åœ¨ä¸æ´»åŠ¨ä½ç½®å±è”½æŸå¤±
        return x, y

def create_datasets(input_file):

    # é¢„å¤„ç†è¾“å…¥æ–‡æœ¬æ–‡ä»¶
    with open(input_file, 'r') as f:
        data = f.read()
    words = data.splitlines()
    words = [w.strip() for w in words] # å»é™¤ä»»ä½•å‰å¯¼æˆ–å°¾éšç©ºæ ¼
    words = [w for w in words if w] # å»é™¤ä»»ä½•ç©ºå­—ç¬¦ä¸²
    chars = sorted(list(set(''.join(words)))) # æ‰€æœ‰å¯èƒ½çš„å­—ç¬¦
    max_word_length = max(len(w) for w in words)
    print(f"æ•°æ®é›†ä¸­çš„ç¤ºä¾‹æ•°é‡ï¼š{len(words)}")
    print(f"æœ€å¤§å•è¯é•¿åº¦ï¼š{max_word_length}")
    print(f"è¯æ±‡è¡¨ä¸­å”¯ä¸€å­—ç¬¦çš„æ•°é‡ï¼š{len(chars)}")
    print("è¯æ±‡è¡¨ï¼š")
    print(''.join(chars))

    # å°†è¾“å…¥æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    test_set_size = min(1000, int(len(words) * 0.1)) # è®­ç»ƒé›†çš„ 10%ï¼Œæˆ–æœ€å¤š 1000 ä¸ªç¤ºä¾‹
    rp = torch.randperm(len(words)).tolist()
    train_words = [words[i] for i in rp[:-test_set_size]]
    test_words = [words[i] for i in rp[-test_set_size:]]
    print(f"å°†æ•°æ®é›†åˆ’åˆ†ä¸º {len(train_words)} ä¸ªè®­ç»ƒç¤ºä¾‹å’Œ {len(test_words)} ä¸ªæµ‹è¯•ç¤ºä¾‹")

    # åŒ…è£…åœ¨æ•°æ®é›†å¯¹è±¡ä¸­
    train_dataset = CharDataset(train_words, chars, max_word_length)
    test_dataset = CharDataset(test_words, chars, max_word_length)

    return train_dataset, test_dataset

class InfiniteDataLoader:
    """
    è¿™çœŸçš„å¾ˆ hackyï¼Œæˆ‘å¯¹æ­¤å¹¶ä¸æ„Ÿåˆ°è‡ªè±ªï¼Œä½†æ˜¯åœ¨ PyTorch ä¸­ä¼¼ä¹æ²¡æœ‰
    æ›´å¥½çš„æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªæ— é™çš„ dataloaderï¼Ÿ
    """

    def __init__(self, dataset, **kwargs):
        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))
        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)
        self.data_iter = iter(self.train_loader)

    def next(self):
        try:
            batch = next(self.data_iter)
        except StopIteration: # è¿™åœ¨æŠ€æœ¯ä¸Šåªä¼šåœ¨ 1e10 ä¸ªæ ·æœ¬ä¹‹åå‘ç”Ÿ...ï¼ˆå³åŸºæœ¬ä¸Šæ°¸è¿œä¸ä¼šï¼‰
            self.data_iter = iter(self.train_loader)
            batch = next(self.data_iter)
        return batch

# -----------------------------------------------------------------------------

class Config:
    """é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨æ‰€æœ‰è®­ç»ƒå‚æ•°"""
    def __init__(self):
        # ä»å…¨å±€å˜é‡ä¸­è¯»å–é…ç½®
        self.input_file = INPUT_FILE
        self.work_dir = WORK_DIR
        self.resume = RESUME
        self.sample_only = SAMPLE_ONLY
        self.num_workers = NUM_WORKERS
        self.max_steps = MAX_STEPS
        self.device = DEVICE
        self.seed = SEED
        self.top_k = TOP_K
        self.type = MODEL_TYPE
        self.n_layer = N_LAYER
        self.n_head = N_HEAD
        self.n_embd = N_EMBD
        self.n_embd2 = N_EMBD2
        self.batch_size = BATCH_SIZE
        self.learning_rate = LEARNING_RATE
        self.weight_decay = WEIGHT_DECAY
    
    def get_relevant_params(self):
        """è¿”å›å½“å‰æ¨¡å‹ç±»å‹ç›¸å…³çš„å‚æ•°"""
        model_params = {
            'transformer': ['n_layer', 'n_head', 'n_embd', 'n_embd2'],
            'bigram': [],  # bigramåªéœ€è¦vocab_sizeï¼Œä¼šè‡ªåŠ¨ä»æ•°æ®è·å–
            'mlp': ['n_embd', 'n_embd2'],
            'rnn': ['n_embd', 'n_embd2'],
            'gru': ['n_embd', 'n_embd2'],
            'bow': ['n_embd', 'n_embd2']
        }
        return model_params.get(self.type, [])
    
    def get_unused_params(self):
        """è¿”å›å½“å‰æ¨¡å‹ç±»å‹ä¸ä½¿ç”¨çš„å‚æ•°"""
        all_params = ['n_layer', 'n_head', 'n_embd', 'n_embd2']
        relevant = self.get_relevant_params()
        return [p for p in all_params if p not in relevant]

# -----------------------------------------------------------------------------
if __name__ == '__main__':

    # åˆ›å»ºé…ç½®å¯¹è±¡
    args = Config()
    
    print("ğŸ”§ å½“å‰é…ç½®è®¾ç½®:")
    print(f"  ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {args.work_dir}")
    print(f"  ğŸ¤– æ¨¡å‹ç±»å‹: {args.type}")
    
    # æ˜¾ç¤ºæ¨¡å‹ç›¸å…³å‚æ•°
    relevant_params = args.get_relevant_params()
    unused_params = args.get_unused_params()
    
    print(f"  ğŸ—ï¸  æ¨¡å‹æ¶æ„å‚æ•°:")
    for param in ['n_layer', 'n_head', 'n_embd', 'n_embd2']:
        value = getattr(args, param)
        if param in relevant_params:
            print(f"     âœ… {param}: {value} (ä½¿ç”¨)")
        elif param in unused_params:
            print(f"     âšª {param}: {value} (å¿½ç•¥)")
    
    print(f"  ğŸƒ è®­ç»ƒå‚æ•°:")
    print(f"     æ‰¹å¤§å°: {args.batch_size}")
    print(f"     å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"     æƒé‡è¡°å‡: {args.weight_decay}")
    print(f"  ğŸ’» ç³»ç»Ÿè®¾ç½®:")
    print(f"     è®¾å¤‡: {args.device}")
    print(f"     æœ€å¤§æ­¥æ•°: {args.max_steps}")
    print(f"     éšæœºç§å­: {args.seed}")
    print("-" * 50)

    # ç³»ç»Ÿåˆå§‹åŒ–
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)
    os.makedirs(args.work_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=args.work_dir)

    # åˆå§‹åŒ–æ•°æ®é›†
    train_dataset, test_dataset = create_datasets(args.input_file)
    vocab_size = train_dataset.get_vocab_size()
    block_size = train_dataset.get_output_length()
    print(f"æ•°æ®é›†ç¡®å®šï¼š{vocab_size=}, {block_size=}")

    # åˆå§‹åŒ–æ¨¡å‹
    config = ModelConfig(vocab_size=vocab_size, block_size=block_size,
                       n_layer=args.n_layer, n_head=args.n_head,
                       n_embd=args.n_embd, n_embd2=args.n_embd2)
    if args.type == 'transformer':
        model = Transformer(config)
    elif args.type == 'bigram':
        model = Bigram(config)
    elif args.type == 'mlp':
        model = MLP(config)
    elif args.type == 'rnn':
        model = RNN(config, cell_type='rnn')
    elif args.type == 'gru':
        model = RNN(config, cell_type='gru')
    elif args.type == 'bow':
        model = BoW(config)
    else:
        raise ValueError(f'model type {args.type} is not recognized')
    model.to(args.device)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡ï¼š{sum(p.numel() for p in model.parameters())}")
    if args.resume or args.sample_only: # æ³¨æ„ï¼šå¦‚æœæˆ‘ä»¬åªé‡‡æ ·ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå‡è®¾æˆ‘ä»¬æ­£åœ¨æ¢å¤
        print("ä»å·¥ä½œç›®å½•ä¸­çš„ç°æœ‰æ¨¡å‹æ¢å¤")
        model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))
    if args.sample_only:
        print_samples(num=50)
        sys.exit()

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)

    # åˆå§‹åŒ– dataloader
    batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)

    # è®­ç»ƒå¾ªç¯
    best_loss = None
    step = 0
    while True:

        t0 = time.time()

        # è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼Œå‘é€åˆ°è®¾å¤‡ï¼Œå¹¶å°†å…¶è§£åŒ…ä¸ºè¾“å…¥å’Œç›®æ ‡
        batch = batch_loader.next()
        batch = [t.to(args.device) for t in batch]
        X, Y = batch

        # è¾“å…¥æ¨¡å‹
        logits, loss = model(X, Y)

        # è®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°æƒé‡
        model.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        # ç­‰å¾… GPU ä¸Šçš„æ‰€æœ‰ CUDA å·¥ä½œå®Œæˆï¼Œç„¶åè®¡ç®—è¿­ä»£æ‰€ç”¨æ—¶é—´
        if args.device.startswith('cuda'):
            torch.cuda.synchronize()
        t1 = time.time()

        # æ—¥å¿—è®°å½•
        if step % 10 == 0:
            print(f"æ­¥éª¤ {step} | æŸå¤± {loss.item():.4f} | æ­¥éª¤æ—¶é—´ {(t1-t0)*1000:.2f}ms")

        # è¯„ä¼°æ¨¡å‹
        if step > 0 and step % 500 == 0:
            train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)
            test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)
            writer.add_scalar("Loss/train", train_loss, step)
            writer.add_scalar("Loss/test", test_loss, step)
            writer.flush()
            print(f"æ­¥éª¤ {step} è®­ç»ƒæŸå¤±ï¼š{train_loss} æµ‹è¯•æŸå¤±ï¼š{test_loss}")
            # å¦‚æœæ¨¡å‹æœ‰æ‰€æ”¹è¿›ï¼Œåˆ™å°†å…¶ä¿å­˜åˆ°ç£ç›˜
            if best_loss is None or test_loss < best_loss:
                out_path = os.path.join(args.work_dir, "model.pt")
                print(f"æµ‹è¯•æŸå¤± {test_loss} æ˜¯ç›®å‰ä¸ºæ­¢æœ€å¥½çš„ï¼Œå°†æ¨¡å‹ä¿å­˜åˆ° {out_path}")
                torch.save(model.state_dict(), out_path)
                best_loss = test_loss

        # ä»æ¨¡å‹ä¸­é‡‡æ ·
        if step > 0 and step % 200 == 0:
            print_samples(num=10)

        step += 1
        # ç»ˆæ­¢æ¡ä»¶
        if args.max_steps >= 0 and step >= args.max_steps:
            break

