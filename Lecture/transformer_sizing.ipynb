{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 理论模型\n",
    "\n",
    "这个笔记本存储了关于 Transformer 的一堆分析，例如估算 FLOPs、参数数量、峰值内存占用、检查点大小等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_args = {\n",
    "#     'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M 参数\n",
    "#     'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M 参数\n",
    "#     'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M 参数\n",
    "#     'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M 参数\n",
    "# }[model_type]\n",
    "\n",
    "block_size = 1024\n",
    "vocab_size = 50257\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "bias = False\n",
    "assert not bias, \"这个笔记本假设 bias=False，只是为了简化计算\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we see: 124337664, expected: 124337664, match: True\n",
      "name                 params     ratio (%) \n",
      "emebedding/position      786432     0.6325\n",
      "embedding/token        38597376    31.0424\n",
      "embedding              39383808    31.6749\n",
      "attention/ln                768     0.0006\n",
      "attention/kqv           1769472     1.4231\n",
      "attention/proj           589824     0.4744\n",
      "attention               2360064     1.8981\n",
      "mlp/ln                      768     0.0006\n",
      "mlp/ffw                 2359296     1.8975\n",
      "mlp/proj                2359296     1.8975\n",
      "mlp                     4719360     3.7956\n",
      "block                   7079424     5.6937\n",
      "transformer            84953088    68.3245\n",
      "ln_f                        768     0.0006\n",
      "dense                         0     0.0000\n",
      "total                 124337664   100.0000\n"
     ]
    }
   ],
   "source": [
    "def params():\n",
    "    \"\"\"估算模型中的参数数量\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # 词嵌入和位置嵌入\n",
    "    out['emebedding/position'] = n_embd * block_size\n",
    "    out['embedding/token'] = n_embd * vocab_size\n",
    "    out['embedding'] = out['emebedding/position'] + out['embedding/token']\n",
    "\n",
    "    # 注意力模块\n",
    "    out['attention/ln'] = n_embd # 注意，我们的层归一化中 bias=False\n",
    "    out['attention/kqv'] = n_embd * 3*n_embd\n",
    "    out['attention/proj'] = n_embd**2\n",
    "    out['attention'] = out['attention/ln'] + out['attention/kqv'] + out['attention/proj']\n",
    "\n",
    "    # MLP 模块\n",
    "    ffw_size = 4*n_embd # 前馈网络大小\n",
    "    out['mlp/ln'] = n_embd\n",
    "    out['mlp/ffw'] = n_embd * ffw_size\n",
    "    out['mlp/proj'] = ffw_size * n_embd\n",
    "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
    "    \n",
    "    # Transformer 和其他部分\n",
    "    out['block'] = out['attention'] + out['mlp']\n",
    "    out['transformer'] = n_layer * out['block']\n",
    "    out['ln_f'] = n_embd # 最终的层归一化\n",
    "    out['dense'] = 0 # 因为参数共享，这个层使用嵌入层的权重，所以为 0\n",
    "\n",
    "    # 总数\n",
    "    out['total'] = out['embedding'] + out['transformer'] + out['ln_f'] + out['dense']\n",
    "\n",
    "    return out\n",
    "\n",
    "# 将我们的参数计数与 PyTorch 报告的进行比较\n",
    "p = params()\n",
    "params_total = p['total']\n",
    "print(f\"we see: {params_total}, expected: {124337664}, match: {params_total == 124337664}\")\n",
    "# 创建表头\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k,v in p.items():\n",
    "    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 1.49 GB\n",
      "measured with wc -c ckpt.pt: 1542470366\n",
      "fluff ratio: 103.38%\n"
     ]
    }
   ],
   "source": [
    "# 我们现在可以计算每个检查点的大小\n",
    "# 参数以 fp32 存储，AdamW 优化器为每个参数额外有 2 个缓冲区用于统计\n",
    "params_bytes = params_total*4\n",
    "params_and_buffers_bytes = params_bytes + 2*params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes/1e9:.2f} GB\")\n",
    "measured_bytes = 1542470366 # 从 wc -c ckpt.pt 测量得到\n",
    "print(f\"measured with wc -c ckpt.pt: {measured_bytes}\")\n",
    "print(f\"fluff ratio: {measured_bytes/params_and_buffers_bytes*100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以估算 GPU 内存中仅由权重和 AdamW 优化器缓冲区占用的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory ratio taken up just for parameters: 3.73%\n"
     ]
    }
   ],
   "source": [
    "gpu_memory = 40e9 # 40 GB A100 GPU，大约\n",
    "print(f\"memory ratio taken up just for parameters: {params_and_buffers_bytes / gpu_memory * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说，对于这个小型模型，内存占用并不多，大部分内存用于激活值（前向和反向传播）。当然，对于越来越大的模型，这一比例会显著变化。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们估算单次前向传播的 FLOPs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/kqv            3623878656     1.2426\n",
      "attention/scores         1610612736     0.5522\n",
      "attention/reduce         1610612736     0.5522\n",
      "attention/proj           1207959552     0.4142\n",
      "attention                8053063680     2.7612\n",
      "mlp/ffw1                 4831838208     1.6567\n",
      "mlp/ffw2                 4831838208     1.6567\n",
      "mlp                      9663676416     3.3135\n",
      "block                   17716740096     6.0747\n",
      "transformer            212600881152    72.8963\n",
      "dense                   79047426048    27.1037\n",
      "forward_total          291648307200   100.0000\n",
      "backward_total         583296614400   200.0000\n",
      "total                  874944921600   300.0000\n"
     ]
    }
   ],
   "source": [
    "def flops():\n",
    "    # 我们只计算权重的 FLOPs，其他层（LayerNorm、Softmax 等）的计算量几乎可以忽略\n",
    "    # 我们计算实际的 FLOPs，而不是 MACs，因此到处都有 2*\n",
    "    # 对于任何矩阵乘法 A (BxC) @ B (CxD) -> (BxD)，FLOPs 为 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = n_embd // n_head\n",
    "\n",
    "    # 注意力模块\n",
    "    # 1) 投影到 key、query、value\n",
    "    out['attention/kqv'] = 2 * block_size * (n_embd * 3*n_embd)\n",
    "    # 2) 计算注意力分数\n",
    "    out['attention/scores'] = 2 * block_size * block_size * n_embd\n",
    "    # 3) 值的加权聚合 (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out['attention/reduce'] = 2 * n_head * (block_size * block_size * head_size)\n",
    "    # 4) 最后的线性投影\n",
    "    out['attention/proj'] = 2 * block_size * (n_embd * n_embd)\n",
    "    out['attention'] = sum(out['attention/'+k] for k in ['kqv', 'scores', 'reduce', 'proj'])\n",
    "\n",
    "    # MLP 模块\n",
    "    ffw_size = 4*n_embd # 前馈网络大小\n",
    "    out['mlp/ffw1'] = 2 * block_size * (n_embd * ffw_size)\n",
    "    out['mlp/ffw2'] = 2 * block_size * (ffw_size * n_embd)\n",
    "    out['mlp'] = out['mlp/ffw1'] + out['mlp/ffw2']\n",
    "\n",
    "    # Transformer 和其他部分\n",
    "    out['block'] = out['attention'] + out['mlp']\n",
    "    out['transformer'] = n_layer * out['block']\n",
    "    out['dense'] = 2 * block_size * (n_embd * vocab_size)\n",
    "\n",
    "    # 前向、反向、总数\n",
    "    out['forward_total'] = out['transformer'] + out['dense']\n",
    "    out['backward_total'] = 2 * out['forward_total'] # 使用常见的反向 = 2*前向估计\n",
    "    out['total'] = out['forward_total'] + out['backward_total']\n",
    "\n",
    "    return out\n",
    "    \n",
    "# 将我们的参数计数与 PyTorch 报告的进行比较\n",
    "f = flops()\n",
    "flops_total = f['forward_total']\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k,v in f.items():\n",
    "    print(f\"{k:20s} {v:14d} {v/flops_total*100:10.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_flops: 875062886400, flops: 874944921600, ratio: 1.0001\n"
     ]
    }
   ],
   "source": [
    "# 这里是从 PaLM 论文中复制的一个估算公式\n",
    "# 这个公式常用于计算 MFU（模型 FLOPs 利用率）\n",
    "def palm_flops():\n",
    "    \"\"\"根据 PaLM 论文公式估算模型 FLOPs\"\"\"\n",
    "    # 非嵌入模型参数。注意，我们不减去嵌入/位置参数，因为它们是共享的，并在最后一层使用\n",
    "    N = params()['total'] - params()['emebedding/position']\n",
    "    L, H, Q, T = n_layer, n_head, n_embd//n_head, block_size\n",
    "    mf_per_token = 6*N + 12*L*H*Q*T\n",
    "    mf = mf_per_token * block_size\n",
    "    return mf\n",
    "\n",
    "print(f\"palm_flops: {palm_flops():d}, flops: {flops()['total']:d}, ratio: {palm_flops()/flops()['total']:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它们非常相似，这让我对 flops() 函数中的数学计算有了一些信心。现在，A100 在张量核心上被引用为 312TFLOPS bfloat16。那么我们的模型 FLOPs 利用率（MFU）是多少？我用 batch_size 为 20 和 grad_accum 为 5 训练了上面的模型，在单个 A100 GPU 上大约运行 755ms。我们得到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of A100 used: 37.14%\n"
     ]
    }
   ],
   "source": [
    "# 这里是我们目前大致测量的结果\n",
    "batch_size = 20 * 5 # 5 是梯度累积，所以总批大小为 100\n",
    "measured_time = 0.755 # 每次迭代的秒数\n",
    "measured_throughput = batch_size / measured_time\n",
    "flops_achieved = f['total'] * measured_throughput\n",
    "\n",
    "# A100 被引用为在张量核心上运行 bfloat16 时为 312 TFLOPS\n",
    "a100_flops_promised = 312e12\n",
    "\n",
    "# 我们使用的 A100 部分占比：\n",
    "print(f\"fraction of A100 used: {flops_achieved / a100_flops_promised * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为参考，我们希望达到 50% 以上，不仅仅对于单个 GPU，而是对于整个 DDP 运行。所以我们还有一些工作要做，但至少我们距离这个 GPU 可实现的性能只差大约 2 倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to train the model: 3.46 days\n"
     ]
    }
   ],
   "source": [
    "# 最后让我们检查 6ND 近似值作为训练总成本的 FLOPs\n",
    "model_size = params()['total'] # 这是参数数量，N\n",
    "tokens_num = 300e9 # 3000 亿个 token，这是数据集大小，D\n",
    "a100_flops = 312e12 # 312 TFLOPS\n",
    "assumed_mfu = 0.3 # 假设这个模型的 FLOPs 利用率（取上面的 37% 并加上一些 DDP 开销）\n",
    "flops_throughput = a100_flops * 8 * assumed_mfu # 假设一个 8xA100 节点，30% 利用率\n",
    "flops_needed = 6 * model_size * tokens_num # 6ND\n",
    "time_needed_s = flops_needed / flops_throughput # 以秒为单位\n",
    "print(f\"time needed to train the model: {time_needed_s/3600/24:.2f} days\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个估算一点也不差。我训练了这个模型，它大约在 4 天内收敛。顺便说一句，关于 6ND 的来源和一些直观理解，我推荐 [Dzmitry 的帖子](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4)。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，FLOPs 只是一种约束，我们还需要密切关注内存带宽。TODO 稍后估算我们模型的 LOAD/STORE 成本。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
